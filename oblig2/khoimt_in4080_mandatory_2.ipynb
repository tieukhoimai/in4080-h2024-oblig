{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080: obligatory assignment 2 (Autumn 2024)\n",
    " \n",
    "Mandatory assignment 1 consists of three parts. In Part 1, you will experiment with a greedy sequence labeling model and investigate the impact of different feature types on **part-of-speech tagging** performance (9 points + 2 optional bonus points). In Part 2, you will evaluate the best model on a different task, **named entity recognition** (5 points). In Part 3, you will return to the **text classification task** and implement a simple feed-forward neural network for it (6 points).\n",
    "\n",
    "You should answer all three parts. You are required to get at least 12/20 points to pass. The most important is that you try to answer each question (possibly with some mistakes), to help you gain a better and more concrete understanding of the topics covered during the lectures. In the first part, there are also bonus questions for those of you who would like to deepen their understanding of the topics covered by this assignment.\n",
    "\n",
    "- We assume that you have read and are familiar with IFI’s requirements and guidelines for mandatory assignments, see [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html) and [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html).\n",
    "- This is an individual assignment. You should not deliver joint submissions. \n",
    "- You may redeliver in Devilry before the deadline (__Sunday, September 29 at 23:59__), but include all files in the last delivery.\n",
    "- Only the last delivery will be read! If you deliver more than one file, put them into a zip-archive. You don't have to include in your delivery the files already provided for this assignment. \n",
    "- Name your submission _your\\_username\\_in4080\\_mandatory\\_2_\n",
    "- You can work on this assignment either on the IFI machines or on your own computer. \n",
    "\n",
    "*The preferred format for the assignment is a completed version of this Jupyter notebook*, containing both your code and explanations about the steps you followed. We want to stress that simply submitting code is __not__ by itself sufficient to complete the assignment - we expect the notebook to also contain explanations of what you have implemented, along with motivations for the choices you made along the way. Preferably use whole sentences, and mathematical formulas if necessary. Explaining in your own words (using concepts we have covered through in the lectures) what you have done and reflecting on your solution is an important part of the learning process - take it seriously!\n",
    "\n",
    "Regarding the use of LLMs (ChatGPT or similar): you are allowed to use them as 'sparring partner', for instance to clarify something you have not understood. However, you are __not__ allowed to use them to generate solutions (either in part or in full) to the assignment tasks.\n",
    "\n",
    "In this assignment, we'll use the following Python modules: `scikit-learn, pyconll, matplotlib, sentence_transformers`. Make sure you have installed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages (1.5.1)\n",
      "Collecting pyconll\n",
      "  Downloading pyconll-3.2.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.53.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (162 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting transformers<5.0.0,>=4.38.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages (from sentence_transformers) (4.66.5)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.4.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting huggingface-hub>=0.19.3 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.19.3->sentence_transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.19.3->sentence_transformers)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.19.3->sentence_transformers)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.19.3->sentence_transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.19.3->sentence_transformers)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting sympy (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.38.0->sentence_transformers)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.38.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.38.0->sentence_transformers)\n",
      "  Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.19.3->sentence_transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.19.3->sentence_transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.19.3->sentence_transformers)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.19.3->sentence_transformers)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading pyconll-3.2.0-py3-none-any.whl (27 kB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "Using cached contourpy-1.3.0-cp312-cp312-macosx_11_0_arm64.whl (251 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.53.1-cp312-cp312-macosx_11_0_arm64.whl (2.2 MB)\n",
      "Downloading huggingface_hub-0.25.0-py3-none-any.whl (436 kB)\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-macosx_11_0_arm64.whl (63 kB)\n",
      "Using cached pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Downloading torch-2.4.1-cp312-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2024.9.11-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl (381 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, sympy, setuptools, safetensors, regex, pyyaml, pyparsing, pyconll, pillow, networkx, MarkupSafe, kiwisolver, idna, fsspec, fonttools, filelock, cycler, contourpy, charset-normalizer, certifi, requests, matplotlib, jinja2, torch, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2024.8.30 charset-normalizer-3.3.2 contourpy-1.3.0 cycler-0.12.1 filelock-3.16.1 fonttools-4.53.1 fsspec-2024.9.0 huggingface-hub-0.25.0 idna-3.10 jinja2-3.1.4 kiwisolver-1.4.7 matplotlib-3.9.2 mpmath-1.3.0 networkx-3.3 pillow-10.4.0 pyconll-3.2.0 pyparsing-3.1.4 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 sentence_transformers-3.1.1 setuptools-75.1.0 sympy-1.13.3 tokenizers-0.19.1 torch-2.4.1 transformers-4.44.2 typing-extensions-4.12.2 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "# if you use a virtual environment, you can install the modules like this\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn pyconll matplotlib sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 – Greedy logistic regression taggers and feature engineering\n",
    "\n",
    "In the lecture, we have discussed Matthew Honnibal's proposal of a discriminative sequence labelling model with greedy decoding. He argued that an extended set of features is more helpful for tagging than exact (Viterbi) decoding. Therefore, we skip HMMs and the Viterbi algorithm here and focus on models based on logistic regression.\n",
    "\n",
    "Scikit-learn doesn't contain implementations of sequence labeling models. Therefore, we provide you with some basic code. The code below defines a `GreedyTagger` with a `fit()` function for training and a `predict()` function for prediction/testing. By default, it uses a standard Scikit-learn logistic regression classifier under the hood, which takes a feature vector for a word (and its context) as input and provides the most likely class label as output.\n",
    "\n",
    "Your task in this part will mainly consist in defining how a word (and its context) is converted to a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "class GreedyTagger:\n",
    "    \"\"\"\n",
    "        Tagger based on logistic regression or any other classification algorithm supported by Scikit-Learn.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_function, classifier=LogisticRegression()):\n",
    "        \"\"\"\n",
    "            Creates a GreedyTagger object.\n",
    "\n",
    "            Args:\n",
    "                feature_function: A function that transforms the input sequence and the index of the current word into a set of key-value pairs.\n",
    "                classifier: A Scikit-Learn classifier instance.\n",
    "        \"\"\"\n",
    "        self.features = feature_function\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        self.label_ids = {}     # mapping from labels to numeric label ids\n",
    "        self.id_labels = {}     # mapping from numeric label ids to labels\n",
    "\n",
    "    def fit(self, dataset_X, dataset_Y):\n",
    "        \"\"\"\n",
    "            Trains the tagger. Creates a list of inputs (feature vectors of individual words) and a list of labels (numeric ids) and calls the `fit` function of the base classifier on these lists.\n",
    "\n",
    "            Args:\n",
    "                dataset_X: The input training data, formatted as a list of lists. Each list item represents a sentence and corresponds to a list of tokens.\n",
    "                dataset_Y: The training data labels, formatted as a list of lists. Each list item represents a sentence and corresponds to a list of labels (e.g. POS-tags or BIO-tags).\n",
    "        \"\"\"\n",
    "        flattened_dataset_X = []      # a flat list of training instances\n",
    "        flattened_dataset_Y = []      # a flat list of label ids for the training instances\n",
    "        for sentence_X, sentence_Y in zip(dataset_X, dataset_Y):\n",
    "            # sentence_X is a list of words\n",
    "            # sentence_Y is a list of labels, one for each word of the sentence\n",
    "            history = []\n",
    "            for i, (x, y) in enumerate(zip(sentence_X, sentence_Y)):\n",
    "                feature_dict = self.features(sentence_X, i, history)\n",
    "                flattened_dataset_X.append(feature_dict)\n",
    "                if y not in self.label_ids:\n",
    "                    self.label_ids[y] = len(self.label_ids)\n",
    "                flattened_dataset_Y.append(self.label_ids[y])\n",
    "                history.append(y)\n",
    "        transformed_dataset_X = self.vectorizer.fit_transform(flattened_dataset_X)\n",
    "        transformed_dataset_Y = np.array(flattened_dataset_Y)\n",
    "        self.id_labels = {self.label_ids[label]: label for label in self.label_ids}\n",
    "        self.classifier.fit(transformed_dataset_X, transformed_dataset_Y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, dataset_X):\n",
    "        \"\"\"\n",
    "            Predicts the labels for the input data, sentence by sentence.\n",
    "\n",
    "            Args:\n",
    "                dataset_X: The input training data, formatted as a list of lists. Each list item represents a sentence and corresponds to a list of tokens.\n",
    "            \n",
    "            Returns:\n",
    "                list: The predicted labels, in the same format as dataset_X (list of lists).\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for sentence_X in dataset_X:\n",
    "            flattened_sentence_X = []\n",
    "            history = []\n",
    "            for i, word in enumerate(sentence_X):\n",
    "                feature_dict = self.features(sentence_X, i, history)\n",
    "                flattened_sentence_X.append(feature_dict)\n",
    "            transformed_sentence_X = self.vectorizer.transform(flattened_sentence_X)\n",
    "            predicted_sentence_Y = self.classifier.predict(transformed_sentence_X)\n",
    "            predicted_labels_Y = [self.id_labels[y] for y in predicted_sentence_Y]\n",
    "            predictions.append(predicted_labels_Y)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When instantiating this class, you need to provide a so-called *feature function* that defines how the feature vector is created. In the provided implementation, this is done in two steps:\n",
    "1. The feature function creates a feature dictionary. Each key of the dictionary defines a one-hot vector, and the value determines which value of the vector will be set to one. For example, the feature dictionary `{\"curr_word\": \"love\", \"prev_word\": \"I\", \"next_word\": \"fish\"}` defines three one-hot vectors, which together represent the word `love` in the sequence `I love fish`. The keys can be chosen freely.\n",
    "2. The `DictVectorizer` class of Scikit-Learn will convert the feature dictionary to actual one-hot vectors and concatenate them into a single vector. This step is already taken care of in the `fit()` function.\n",
    "\n",
    "A basic feature function that only uses the current word could look like this (we will use the parameter `history` later, but don't worry about it now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_features(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict()` function only returns the predicted labels, it doesn't compare them to the ground truth. Here is another function that computes accuracy for the predictions of a dataset. However, the function is incomplete -- it only uses the first sentence of the dataset.\n",
    "\n",
    "**Task 1.1** (1 point): Modify the function to take into account all sentences of the dataset. The easiest way to achieve this is to flatten a list of lists into a single list, so you'll only have to call the `accuracy_score` function once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging_accuracy(dataset_Y_true, dataset_Y_pred):\n",
    "\t## CHANGE CODE HERE\n",
    "\twords_Y_true = [label for sentence in dataset_Y_true for label in sentence]\n",
    "\twords_Y_pred = [label for sentence in dataset_Y_pred for label in sentence]\n",
    "\n",
    "\tacc = sklearn.metrics.accuracy_score(words_Y_true, words_Y_pred)\n",
    "\n",
    "\treturn acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need some actual data to work with. We provide you with part-of-speech annotated data for Norwegian Bokmål taken from (https://universaldependencies.org/). To load the data, you can use the `pyconll` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15696 sentences loaded from file no_bokmaal-ud-train.conllu\n",
      "2409 sentences loaded from file no_bokmaal-ud-dev.conllu\n",
      "1939 sentences loaded from file no_bokmaal-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "import pyconll\n",
    "\n",
    "def load_data(filename):\n",
    "\tdata = pyconll.load_from_file(filename)\n",
    "\tX, Y = [], []\n",
    "\tfor sentence in data:\n",
    "\t\tX.append([token.form for token in sentence])\t# the \"form\" field contains the words\n",
    "\t\tY.append([token.upos for token in sentence])\t# the \"upos\" field contains the part-of-speech tags in universal POS format\n",
    "\tprint(f\"{len(X)} sentences loaded from file {filename}\")\n",
    "\treturn X, Y\n",
    "\n",
    "train_data_x, train_data_y = load_data('no_bokmaal-ud-train.conllu')\n",
    "valid_data_x, valid_data_y = load_data('no_bokmaal-ud-dev.conllu')\n",
    "test_data_x, test_data_y = load_data('no_bokmaal-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can put everything together. Let us train a tagger on the training data and evaluate it on the validation set.\n",
    "\n",
    "**Task 1.2** (1 point): Write the corresponding code and output the accuracy (as a number between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8643\n",
      "Test Accuracy: 0.8608\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training set\n",
    "tagger = GreedyTagger(feature_function=basic_features)\n",
    "tagger.fit(train_data_x, train_data_y)\n",
    "\n",
    "valid_predictions = tagger.predict(valid_data_x)\n",
    "valid_accuracy = tagging_accuracy(valid_data_y, valid_predictions)\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "test_predictions = tagger.predict(test_data_x)\n",
    "test_accuracy = tagging_accuracy(test_data_y, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3** (1 point): The basic feature function only looks at the current word. Also add the previous word, the next word, and the word before the previous one. Make sure to handle edge cases at the beginning and at the end of the sentence. For example, the feature dictionary for the first word of `I love fish` could look as follows: `{\"curr_word\": \"I\", \"prev_word\": \"<START>\", \"prev2_word\": \"<START>\", \"next_word\": \"love\"}` (you should still not need the parameter `history` for this). Add one feature at a time, train a model, and record the accuracy. Describe which combination of features works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_features(sentence, i, history, use_prev_word=False, use_next_word=False, use_prev2_word=False):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if use_prev_word:\n",
    "        features[\"prev_word\"] = sentence[i-1] if i > 0 else \"<START>\"\n",
    "    if use_prev2_word:\n",
    "        features[\"prev2_word\"] = sentence[i-2] if i > 1 else \"<START>\"\n",
    "    if use_next_word:\n",
    "        features[\"next_word\"] = sentence[i+1] if i < len(sentence)-1 else \"<END>\"\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of feature configurations\n",
    "feature_configs = [\n",
    "    (\"Current Word\", {\"use_prev_word\": False, \"use_next_word\": False, \"use_prev2_word\": False}),\n",
    "    (\"Current Word + Previous Word\", {\"use_prev_word\": True, \"use_next_word\": False, \"use_prev2_word\": False}),\n",
    "    (\"Current Word + Previous Word + Next Word\", {\"use_prev_word\": True, \"use_next_word\": True, \"use_prev2_word\": False}),\n",
    "    (\"Current Word + Previous Word + Next Word + Previous-2 Word\", {\"use_prev_word\": True, \"use_next_word\": True, \"use_prev2_word\": True})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(feature_config):\n",
    "    tagger = GreedyTagger(feature_function=lambda sentence, i, history: context_features(sentence, i, history, **feature_config))\n",
    "    tagger.fit(train_data_x, train_data_y)\n",
    "    valid_predictions = tagger.predict(valid_data_x)\n",
    "    valid_accuracy = tagging_accuracy(valid_data_y, valid_predictions)\n",
    "    return valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Word: 0.8643\n",
      "Current Word + Previous Word: 0.8908\n",
      "Current Word + Previous Word + Next Word: 0.9178\n",
      "Current Word + Previous Word + Next Word + Previous-2 Word: 0.9192\n",
      "Best Feature Combination: Current Word + Previous Word + Next Word + Previous-2 Word with Accuracy: 0.9192\n"
     ]
    }
   ],
   "source": [
    "accuracies = {}\n",
    "for name, config in feature_configs:\n",
    "    accuracy = train_and_evaluate(config)\n",
    "    accuracies[name] = accuracy\n",
    "    print(f\"{name}: {accuracy:.4f}\")\n",
    "\n",
    "best_feature_combination = max(accuracies, key=accuracies.get)\n",
    "print(f\"Best Feature Combination: {best_feature_combination} with Accuracy: {accuracies[best_feature_combination]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4** (1 point): Up to now, we are still using a unigram model, in the sense that no information about the *labels* at previous positions is incorporated. Let us change this now by adding so-called *transition features*. Add a key `prev_tag` to the feature function and fill it with the label of the previous position. You can use the `history` parameter for this. Write your new feature function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_features(sentence, i, history):\n",
    "    features = {\n",
    "        \"curr_word\": sentence[i],\n",
    "        \"prev_word\": sentence[i-1] if i > 0 else \"<START>\",\n",
    "        \"prev2_word\": sentence[i-2] if i > 1 else \"<START>\",\n",
    "        \"next_word\": sentence[i+1] if i < len(sentence)-1 else \"<END>\",\n",
    "        \"prev_tag\": history[-1] if history else \"\"\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9016\n",
      "Test Accuracy: 0.8942\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training set\n",
    "tagger = GreedyTagger(feature_function=transition_features)\n",
    "tagger.fit(train_data_x, train_data_y)\n",
    "\n",
    "valid_predictions = tagger.predict(valid_data_x)\n",
    "valid_accuracy = tagging_accuracy(valid_data_y, valid_predictions)\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "test_predictions = tagger.predict(test_data_x)\n",
    "test_accuracy = tagging_accuracy(test_data_y, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start training the model with the transition features, have a closer look at the starter code provided at the beginning. You will notice that the `history` parameter is correctly filled in the `fit()` function, but that it always remains empty in the `predict()` function. With this setup, adding transition features will not have any impact on the accuracy. You will therefore have to implement an alternative prediction function that fixes this issue.\n",
    "\n",
    "**Task 1.5** (2 points): Complete the `predict_with_history()` function below such that the history list is filled with the predicted label at each step. Instead of calling `self.classifier.predict()` once per sentence, you will have to call it for each word separately, because the feature vector for a given position can only be computed once the previous word has been labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_history(self, dataset_X):\n",
    "\tpass\n",
    "\n",
    "# attach the function to the GreedyTagger class\n",
    "setattr(GreedyTagger, 'predict_with_history', predict_with_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.6** (1 point): Train a new tagger using the transition features and the `predict_with_history()` function. Do the transition features help? Note: The prediction will be significantly slower than before because of the modifications made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.7** (_optional, 2 extra points_): You may have noticed that there is a significant implementation difference between the `fit()` and `predict_with_history()` functions. During training, the `history` list is filled with the *gold* labels directly taken from the annotated training data. During prediction, the gold labels are not available, so we have to resort to the *predicted* labels. Could this approach lead to any problems? If so, how could it be improved? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.8** (_optional, 2 extra points_): Take the currently best-performing feature function and add even more features to get an even better tagger. Some ideas:\n",
    "- Extract suffixes and prefixes of $n$ characters ($1 \\leq n \\leq 4$) from the current, previous or next word.\n",
    "- Is the current word a number?\n",
    "- Is it capitalized?\n",
    "- Does it contain capitals?\n",
    "- Does it contain a hyphen? etc.\n",
    "\n",
    "What is the best feature set you can come up with? Define various feature sets and select the best one based on validation set accuracy.\n",
    "If you use sources for finding tips about good features (like articles, web pages, etc.) make references to the sources and explain what you got from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have been working on the development set. It is time now to evaluate your best-performing model on the held-out test set.\n",
    "\n",
    "**Task 1.9** (2 points): First, compute the *test set accuracy* of your best model. However, the accuracy only gives us a high-level overview of the performance of a tagger, but we may be interested in finding out more details about where the tagger makes the mistakes. The universal tagset is reasonably small, so we can produce a *confusion matrix*. Take a look at https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html and generate a confusion matrix for the results. Which pairs of tags are most easily confounded? You can find the documentation of the tagset in the following link: https://universaldependencies.org/u/pos/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 – Span identification with sequence models\n",
    "\n",
    "In this part, you'll use the same sequence models for a different task: named entity recognition for English. The following code loads the training and test files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data(filename):\n",
    "\tdataset_X, dataset_Y = [], []\n",
    "\tsent_X, sent_Y = [], []\n",
    "\twith open(filename, 'r') as ner_file:\n",
    "\t\tfor line in ner_file:\n",
    "\t\t\tif line.strip() == \"\":\n",
    "\t\t\t\tif len(sent_X) > 0:\n",
    "\t\t\t\t\tdataset_X.append(sent_X)\n",
    "\t\t\t\t\tdataset_Y.append(sent_Y)\n",
    "\t\t\t\t\tsent_X = []\n",
    "\t\t\t\t\tsent_Y = []\n",
    "\t\t\telif line.startswith(\"#\") or line.startswith(\"-DOCSTART-\"):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\telements = line.strip().split(\" \")\n",
    "\t\t\t\tsent_X.append(elements[0])\n",
    "\t\t\t\tsent_Y.append(elements[3])\n",
    "\t\tif len(sent_X) > 0:\n",
    "\t\t\tdataset_X.append(sent_X)\n",
    "\t\t\tdataset_Y.append(sent_Y)\n",
    "\tprint(f\"{len(dataset_X)} sentences loaded from file {filename}\")\n",
    "\treturn dataset_X, dataset_Y\n",
    "\n",
    "train_nerdata_x, train_nerdata_y = load_ner_data('ner-train.txt')\n",
    "test_nerdata_x, test_nerdata_y = load_ner_data('ner-test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1** (1 point): Train a greedy tagger with the NER training data. Use any feature set that worked well for POS tagging, but *do not include transition features* for now. Compute the token-level accuracy of the test set, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2** (1 point): Token-level accuracy is not well adapted to span identification tasks (why?). Instead, we want to compute precision, recall and f-score of the *entities*. Adapt the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranges(l):\n",
    "    \"\"\"\n",
    "        Helper procedure for eval_ner.\n",
    "        You're not expected to change anything here.\n",
    "    \"\"\"\n",
    "    elements = []\n",
    "    current_element = None\n",
    "    current_start = None\n",
    "    for i, ll in enumerate(l):\n",
    "        if ll == 'O':\n",
    "            if current_element != None:\n",
    "                elements.append((current_element, current_start, i))\n",
    "            current_element = None\n",
    "            current_start = -1\n",
    "        elif ll[0] == 'B':\n",
    "            if current_element != None:\n",
    "                elements.append((current_element, current_start, i))\n",
    "            current_element = ll[2:]\n",
    "            current_start = i\n",
    "        elif ll[0] == \"I\":\n",
    "            if current_element != ll[2:]:\n",
    "                elements.append((current_element, current_start, i))\n",
    "                current_element = ll[2:]\n",
    "                current_start = i\n",
    "    return elements\n",
    "\n",
    "def eval_ner(sys_labels, gold_labels):\n",
    "    \"\"\"\n",
    "        Computes precision, recall and f1-score, using get_ranges() to identify the named entities.\n",
    "        No need to edit this function.\n",
    "    \"\"\"\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for sys_l, gold_l in zip(sys_labels, gold_labels):\n",
    "        sys_ranges = get_ranges(sys_l)\n",
    "        gold_ranges = get_ranges(gold_l)\n",
    "        for r in sys_ranges:\n",
    "            if r in gold_ranges:\n",
    "                tp += 1\n",
    "        ## TODO: also count fp and fn\n",
    "    \n",
    "    recall = 0\n",
    "    prec = 0\n",
    "    f1score = 0\n",
    "    ## TODO: compute recall, precision and f1-score from tp, fp and fn\n",
    "\n",
    "    return recall, prec, f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report precision, recall and f1-score of the model trained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3** (1 point): Train a new model with transition features and predict the development set labels using `predict_with_history`. How does this change affect the token-level (accuracy) and entity-level (recall, precision, f1-score) metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4** (2 points): Take some 50 sentences from the test set and display the words, gold tags, and the predicted tags of the two systems. Can you identify error patterns that are typical for the two systems respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text classification with feed-forward neural networks\n",
    "\n",
    "For this part, we go back to the text classification task used in the first assignment: language identification of Bokmål and Nynorsk.\n",
    "\n",
    "Instead of manually creating a bag-of-subwords matrix with the BPE-encoded data, we go a simpler route this time and use the `CountVectorizer` class provided by Scikit-Learn. Have a look at the documentation of `CountVectorizer` here: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "With the default parameters, `CountVectorizer` will tokenize the data at whitespaces, remove punctuation signs and convert everything to lowercase. You can play around with the parameters if you wish, but the default settings work decently well. A useful thing to make the vectors smaller is to remove hapaxes, i.e. words that occur only once in the training data. You can achieve this with `min_df=2`.\n",
    "\n",
    "**Task 3.1** (2 points): Write the code to produce `train_X, train_Y, test_X, test_Y`. Each `*_X` variable should contain a list of vectors produced by `CountVectorizer`. Each `*_Y` variable should contain a list of integers (1 for Nynorsk and 0 for Bokmål)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2** (1 point): Create a logistic regression classifier, train it on the training set, and predict the labels of the test set. Compute accuracy, precision and recall, as in the previous assignment. How do your results compare with the previous assignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3** (1 point): Let us use a simple feed-forward neural network classifier instead of logistic regression, but keep the input feature representation identical. Scikit-Learn provides a simple feed-forward classifier under the name `MLPClassifier`. The most important parameter is `hidden_layer_sizes`, which specifies the number and size of hidden layers. For example, `MLPClassifier(hidden_layer_sizes=(10, 5))` creates a classifier with two hidden layers, the first one with 10 neurons and the second one with 5 neurons. To start, keep the model simple and choose a single hidden layer with 10 neurons. How does this model perform in comparison with logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.4** (1 point): Try to improve the results by adding more and/or larger hidden layers. You're not expected to work with more than 50 neurons in total, as this slows down the training process drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see if we can improve the input representation. Instead of a bag-of-words model created with `CountVectorizer`, we will obtain document embeddings from a pretrained Norwegian SentenceBert model (in particular, the [base model from the National Library of Norway](https://huggingface.co/NbAiLab/nb-sbert-base)). The following snippet shows how to load and use such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('NbAiLab/nb-sbert-base')\n",
    "sentences = [\"This is a Norwegian boy\", \"Dette er en norsk gutt\", \"Dette er ein norsk gut\"]\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.5** (1 point): Produce embeddings for the training and test sets and train a new `MLPClassifier` with these embeddings as input features. Can you further improve the evaluation scores?\n",
    "\n",
    "Note: Producing the embeddings and training the model will take longer than in previous exercises, around 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
