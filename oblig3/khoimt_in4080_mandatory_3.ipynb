{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080: obligatory assignment 3\n",
    " \n",
    "Mandatory assignment 3 is about the practical use of Large Language Models (LLMs). More specifically, you will be tasked to implement a RAG (Retrieval-Augmented Generation) system able to answer factual questions based on a document database, more specifically Wiki pages extracted from an [online Star Wars encyclopedia](https://starwars.fandom.com). \n",
    "\n",
    "You are required to get at least 12/20 points to pass. \n",
    "\n",
    "- We assume that you have read and are familiar with IFI’s requirements and guidelines for mandatory assignments, see [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html) and [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html).\n",
    "- This is an individual assignment. You should not deliver joint submissions. \n",
    "- You may redeliver in Devilry before the deadline (__Sunday, October 13 at 23:59__).\n",
    "- Only the last delivery will be read! If you deliver more than one file, put them into a zip-archive. You don't have to include in your delivery the data files already provided for this assignment. \n",
    "- Name your submission _your\\_username\\_in4080\\_mandatory\\_3_\n",
    "\n",
    "The preferred way to complete this assignment is using the high-performance computing cluster _Fox_. See [here](https://www.uio.no/studier/emner/matnat/ifi/IN4080/h24/computing-setup.html) for instructions on how to register and log in to Fox.\n",
    "\n",
    "You should deliver a completed version of this Jupyter notebook, containing both your code and explanations about the steps you followed. We want to stress that simply submitting code is __not__ by itself sufficient to complete the assignment - we expect the notebook to also contain explanations of what you have implemented, along with motivations for the choices you made along the way. Preferably use whole sentences, and mathematical formulas if necessary. Explaining in your own words (using concepts we have covered through in the lectures) what you have implemented and reflecting on your solution is an important part of the learning process - take it seriously!\n",
    "\n",
    "Regarding the use of LLMs (ChatGPT or similar): you are allowed to use them as 'sparring partner', for instance to clarify something you have not understood. However, you are __not__ allowed to use them to generate solutions (either in part or in full) to the assignment tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "\n",
    "We will start by building a chatbot that directly answers user questions using an instruction-tuned LLM, without relying on any database. We will use the instruction-tuned version of the [Gemma 1.1 language model](https://huggingface.co/google/gemma-1.1-2b-it) from Google, which is available on HuggingFace. \n",
    "\n",
    "_Note: feel free to switch to another model (such as the newly released Llama 3 models) if you wish to experiment with them. Note, however, that the most recent LLMs will likely require a newer version of the `transformers` library than what is currently installed on Fox._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1** (4 points): Drawing inspiration from the code examples on the [Gemma webpage](https://huggingface.co/google/gemma-1.1-2b-it), implement the `__init__` and `get_response` methods. If you run the code on Fox with a GPU (or on a personal machine with a GPU), make sure that your code actually runs on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# loading the HUGGINGFACE_TOKEN Keys from .env file\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "hugging_face_token = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fc35c3fe954d248b159a4d84a932fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class BasicResponseGenerator:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemma-1.1-2b-it\"):\n",
    "        \"\"\"Loads the tokenizer and pretrained causal LM for the given model. \n",
    "        If a GPU is available, the model should be loaded on the GPU \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, token=hugging_face_token)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            token=hugging_face_token\n",
    "        )\n",
    "\n",
    "    def get_response(self, prompt:str, max_length:int=50) -> str:\n",
    "        \"\"\"Given a prompt, generate a response (of a maximum max_length tokens) and return it.\n",
    "        Only the response should be returned, not the text of the prompt itself\n",
    "        \"\"\"\n",
    "        \n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(**input_ids,max_length=max_length)\n",
    "\n",
    "        return self.tokenizer.decode(outputs[0])\n",
    "\n",
    "agent = BasicResponseGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: An easy way to verify that the GPU is actually used is to run the command `nvidia-smi` while your code is running. There also exists alternative GPU monitoring tools, like [`gpustat`](https://pypi.org/project/gpustat/0.3.2/)._\n",
    "\n",
    "You can then test your response generator with the following set of questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Luke Skywalker?\n",
      "Answer: <bos>Who is Luke Skywalker?\n",
      "\n",
      "Luke Skywalker is a fictional character in the Star Wars franchise, a member of the Skywalker family, and a key figure in the Star Wars saga. He is the son of Anakin Skywalker and Padmé Amidala, and\n",
      "-------\n",
      "Question: Where is the Niima Outpost in Star Wars?\n",
      "Answer: <bos>Where is the Niima Outpost in Star Wars?\n",
      "\n",
      "The Niima Outpost is not mentioned in the Star Wars universe, so it does not exist.<eos>\n",
      "-------\n",
      "Question: Have you heard of Nute Gunray? Who is he?\n",
      "Answer: <bos>Have you heard of Nute Gunray? Who is he?\n",
      "\n",
      "I am unable to find any information about Nute Gunray on the internet.<eos>\n",
      "-------\n",
      "Question: What kind of planet is Kashyyyk, and who discovered it?\n",
      "Answer: <bos>What kind of planet is Kashyyyk, and who discovered it?\n",
      "\n",
      "**Kashyyyk** is a fictional planet from the Star Wars universe. It is a forested planet located in the Outer Rim.\n",
      "\n",
      "**Kashyyyk was discovered by\n",
      "-------\n",
      "Question: Who are Condlurans, and can you give 2-3 names of known Condlurans?\n",
      "Answer: <bos>Who are Condlurans, and can you give 2-3 names of known Condlurans?\n",
      "\n",
      "The term \"Condlurans\" is derived from the Greek word \"kondilos,\" which means \"to scrape.\"\n",
      "\n",
      "**Answer:**\n",
      "-------\n",
      "Question: What can you tell me about the First Battle of Geonosis?\n",
      "Answer: <bos>What can you tell me about the First Battle of Geonosis?\n",
      "\n",
      "The First Battle of Geonosis was a pivotal battle in the Clone Wars, fought on the planet Geonosis. It was a decisive victory for the Republic, led\n",
      "-------\n",
      "Question: What is the name of the settlement where Anakin Skywalker and his mother lived?\n",
      "Answer: <bos>What is the name of the settlement where Anakin Skywalker and his mother lived?\n",
      "\n",
      "Anakin Skywalker and his mother lived on the planet **Babel-Babel**.<eos>\n",
      "-------\n",
      "Question: Which planet did Darth Sidious represent as senator?\n",
      "Answer: <bos>Which planet did Darth Sidious represent as senator?\n",
      "\n",
      "This question cannot be answered because Darth Sidious is a fictional character and does not represent any real-world entity.<eos>\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "questions = [\"Who is Luke Skywalker?\",\n",
    "             \"Where is the Niima Outpost in Star Wars?\",\n",
    "             \"Have you heard of Nute Gunray? Who is he?\",\n",
    "             \"What kind of planet is Kashyyyk, and who discovered it?\",\n",
    "             \"Who are Condlurans, and can you give 2-3 names of known Condlurans?\",\n",
    "             \"What can you tell me about the First Battle of Geonosis?\",\n",
    "             \"What is the name of the settlement where Anakin Skywalker and his mother lived?\",\n",
    "             \"Which planet did Darth Sidious represent as senator?\"]\n",
    "\n",
    "for question in questions:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Answer:\", agent.get_response(question))\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, the model should give you a few correct answers, but also many responses for which the model is either unable to give a precise answer, or hallucinates a (wrong) answer. This is expected, as the model is relatively small (3 billion parameters) and is a generic model that is not particularly optimised to generate trivia about the Star Wars Franchise. We will now try to improve the model performance by coupling the LLM to a document database.\n",
    "\n",
    "## Retrieval step\n",
    "\n",
    "Retrieval-augmented generation operates on a simple idea: instead of directly generating a response based on the \"parametric knowledge\" of the LLM, we first search for relevant documents in a database (or on the web). We then include the most relevant documents to the prompt, and ask the LLM to answer the user question _based on this retrieved knowledge_. \n",
    "\n",
    "In this assignment, you will use a set of Wiki texts extracted from an [online Star Wars encyclopedia](https://starwars.fandom.com) as document database. The wiki texts are available as a JSON file, either [here](https://home.nr.no/~plison/data/starwars.json) or on Fox at `/fp/projects01/ec403/IN4080/starwars.json`. The JSON is simply a dictionary mapping Wiki page titles to their content (in plain text).\n",
    "\n",
    "### Sparse retrieval \n",
    "\n",
    "We can start by using the newly released [BM25s](https://bm25s.github.io/) library, which implements a number of well-known search algorithms, which are all variants of the original [BM25 algorithm](https://en.wikipedia.org/wiki/Okapi_BM25) . Although BM25 is an old-fashioned search technique based on bag-of-words, it remains suprisingly effective, and is still widely used in modern NLP systems.\n",
    "\n",
    "**Task 2** (4 points): Fill in the implementation for the `BM25Retriever` class using [BM25s](https://bm25s.github.io/) (see the library documentation for details). You should filter out stop words by adding `stopwords='en_plus'` to the arguments of the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 64.4M  100 64.4M    0     0  3172k      0  0:00:20  0:00:20 --:--:-- 3062k03 3084k20  0:00:20 --:--:-- 3006k\n"
     ]
    }
   ],
   "source": [
    "!curl https://home.nr.no/~plison/data/starwars.json > starwars.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "class BM25Retriever:\n",
    "\n",
    "    def __init__(self, filename=\"starwars.json\"):\n",
    "        \"\"\"Using the json file provided as input, create a BM25s retriever \n",
    "        containing all (indexed) documents.\"\"\"\n",
    "        with open(filename, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        self.corpus = list(data.values())\n",
    "        self.corpus_tokens = bm25s.tokenize(self.corpus, stopwords=\"en_plus\")\n",
    "\n",
    "        self.retriever = bm25s.BM25(corpus=self.corpus)\n",
    "        self.retriever.index(self.corpus_tokens)\n",
    "        \n",
    "    def search(self, query:str, k:int=5) -> List[str]:\n",
    "        \"\"\"Use the BM25 retriever to find the k documents that are closest\n",
    "        to the provided query\"\"\"\n",
    "\n",
    "        query_tokens = bm25s.tokenize(query)\n",
    "        result = self.retriever.retrieve(query_tokens, k=k, return_as=\"documents\")\n",
    "\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then test our retriever by checking whether the documents with highest BM25 scores are indeed the ones that are most relevant to the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642cfd0cb0c14ea8b3fad7bd6688461a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/178246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a755a0f08a244abd84c648eae93edbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/178246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c87a4c8bd55422b8a2164b1d2592f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/178246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Luke Skywalker?\n",
      "Retrieved documents:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e805708e0e148768cc0f2805c293d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6790e27d07e345e09c9a2a9dcbb0e2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Luke Skywalker's lightsaber was the first lightsaber constructed by Luke Skywalker and the second one he owned.\n",
      "- The Luke Skywalker X-Wing Mech was a mech piloted by Luke Skywalker. It shared a design with his X-wing and included a large lightsaber.\n",
      "- Holoeditor was a type of holographic technology used by Cronal to study and imitate Luke Skywalker in 5 ABY.Luke Skywalker and the Shadows of Mindor\n",
      "- Luke Skywalker and the Jedi's Revenge was a holothriller produced prior to 6 ABY, which purported to depict the duel between Luke Skywalker and Darth Vader aboard the second Death Star. Skywalker took exception to its historical inaccuracies, specifically its depiction of him killing Vader to avenge Palpatine's death at Vader's hands, which he characterized as \"sick\". In actuality the holothriller was created by Blackhole, in order to establish that Luke Skywalker was the next legitimate heir to the Empire. Blackhole was secretly planning on becoming Luke Skywalker during the Battle of Mindor.\n",
      "- Luke's Backpack was a blue colored satchel made on Dantooine out of gundark-skin that belonged to Luke Skywalker. It possessed many pockets, which Skywalker used while on patrol to hold supplies. Skywalker wore it on Dagobah, when he was under Yoda's tutelage. Yoda rode inside the backpack at least once, instructing Luke during a training run.\n",
      "===========\n",
      "Question: Where is the Niima Outpost in Star Wars?\n",
      "Retrieved documents:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60106e7bce254812980d821dd5f070af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27bca7080a1488b95c3fa64936d3cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The Niima Outpost Militia was a law enforcement agency stationed at Niima Outpost on the desert planet of Jakku. It was led by the Kyuzo Constable Zuvio and his two cousins&mdash;Drego and Streehn.\n",
      "- Bay Three was a docking bay in Niima Outpost on the planet Jakku. When the scavenger Rey took BB-8 to Niima Outpost, she told the droid there was a trader in Bay Three named Horvins who may have been willing to give BB-8 a lift offworld.\n",
      "- Niima Outpost was a junkyard settlement on Jakku, a desert planet in the Western Reaches of the galaxy. The outpost was named for and founded by Niima the Hutt after the Battle of Jakku to capitalize on the new scavenging opportunities the battle created on the planet. Niima Outpost was the only spaceport on the planet, although it was referred more as a landing field rather than a spaceport.Rey's Survival Guide Scavengers, like Rey, salvaged materials from the technology leftover from the Battle of Jakku. Salvage made up the backbone of the economy, but other branches such as black market trading, mercenaries for hire, and other unlawful activities exist.  Niima's fuel supplies were basic, though there were some scavengers who located military grade supplies, such as rhydonium. Niima attracted a lot of attention offworld because of it being the only navigational beacon on the planet. These foreigners were encouraged to lock up their ships. Most travelers opted to sleep on their ships due to lack of permanent structures in the outpost. Scavengers brought their finds to Niima Outpost in exchange for supplies from Unkar Plutt, the local boss who operated from the center of awning-roofed blockhouse known as the Concession Stand. Niima Outpost was also the location of Constable Zuvio's office. The outpost's main gate utilized architecture akin to that found on other Hutt worlds such as Teth. Sarco Plank opened a gun shop at the outpost to modify and enhance blasters.  The outpost was attacked by the First Order, who were looking for the astromech droid BB-8 and stormtrooper defector Finn.\n",
      "- The Office of the Constable was a small building used as a headquarters by the Niima Outpost Militia on Jakku.\n",
      "- Niima, also known as Niima the Hutt, was an enterprising female Hutt scoundrel who operated far from the borders of Hutt Space, a province of the galaxy that became embroiled in conflict as the Hutts attempted to carve up the deceased crime lord Jabba Desilijic Tiure's territory. Niima resided in a temple on the desert planet of Jakku. Prior to the Battle of Jakku, Niima struck a deal with Gallius Rax, the Counselor to the Empire, to guard the pass to the Plaintive Hand plateau and kidnap local orphans in return for receiving weapons. Later, Rae Sloane struck a deal with Niima to gain access to the Jakku Observatory, claiming that it hid a powerful weapon there.  Niima was later wounded when Rax ambushed her convoy and captured Sloane and Brentin Lore Wexley. Despite being wounded, Niima survived with the help of the bounty hunter Jas Emari, who ferried her back to her temple. Following the Battle of Jakku, Niima monopolized the salvage collection efforts on Jakku and established Niima Outpost. Though Niima was later killed by a bounty hunter, Niima Outpost continued to bear her name. It was rumored that a group who worshiped the Hutts as gods constructed their village around the cranial mantle of Niima herself, gilded in precious metals and covered with jewels, following her demise.\n",
      "===========\n",
      "Question: Have you heard of Nute Gunray? Who is he?\n",
      "Retrieved documents:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd0c521e3a343a7bca8cbf985881a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6506eb32355404988621cbe81d57c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Nute Gunray's citadel, also referred to as Nute Gunray's redoubt, was a large fortress located on the western hemisphere of Cato Neimoidia. It was used as a stronghold and storehouse by Nute Gunray, Viceroy of the Trade Federation.\n",
      "- Lora Besh claimed to be the secret lover of Trade Federation Viceroy Nute Gunray; her book on the alleged affair, Gunray On Top, was a bestseller in the months preceding the Clone Wars.\n",
      "- The Viceroy's collar was an item worn by the Viceroy of the Trade Federation. Nute Gunray wore the collar.\n",
      "- The sovereign beetle was a species from Neimoidia. The patterns on its shell were the basis of the ornamentation on Nute Gunray's mechno-chair.Cloak of Deception\n",
      "- In 44 BBY, then-Senator Nute Gunray used a Trade Federation shuttle of a different class than the Sheathipede-class transport shuttle.\n",
      "===========\n",
      "Question: What kind of planet is Kashyyyk, and who discovered it?\n",
      "Retrieved documents:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3d47b3f1f348fa8b88300acfa48c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff750cf2d7949148180c40201f84a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Mock Shyr was a kind of broad leafed plant native to Kashyyyk.\n",
      "- The kolvissh was a kind of flowering plant native to Kashyyyk. Mallatobuck once used some for her bridal veil.\n",
      "- A rare and unique spice variant was discovered during the Galactic Civil War. This kind of spice appeared organicQuest: \"Man Down!\" and medicinal in nature.\n",
      "- Kashyyyk, also known as Planet Wookiee C to some humans in the Core Worlds, was a wroshyr tree-covered forest planet located in the southwestern quadrant of the galaxy and the homeworld of the Wookiee species. Four millennia before the Battle of Yavin, Kashyyyk was discovered by the Czerka Corporation, who enslaved the Wookiee population and renamed the planet G5-623, later to Edean. Using superior technology, the company managed to enslave the Wookiees until an uprising drove the oppressors away. During the Clone Wars, Kashyyyk was a member of the Galactic Republic, and endured enslavement under the Galactic Empire, the Republic's successor. Later, during the rise of the New Republic, Kashyyyk was liberated with the help of Republic forces led by Han Solo.\n",
      "- Kashyyyk wheat was a type of wheat grown on the planet of Kashyyyk. It was used to make Kashyyyk wheat flour.\n",
      "===========\n",
      "Question: Who are Condlurans, and can you give 2-3 names of known Condlurans?\n",
      "Retrieved documents:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ceb7d781ff843da986f7134ba0e598b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcfaef1542c4585a0070b808af8ea22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Condlurans were a sentient species that existed within the galaxy. The Condluran Briff worked as a smuggler on Jedha during the High Republic Era.The High Republic (2022) 2 During the Imperial Era, Freck was a Condluran male who worked as a transport driver for the Galactic Empire on the mining planet Mapuzo.\n",
      "- A core name was a shortened name used by Chiss. Members of the Chiss species used their core names rather than their full names for at least two reasons. Among Chiss, core names were used in all but the most formal settings. Chiss also gave their core names to members of other species, as non-Chiss had difficulties pronouncing full Chiss names.\n",
      "- Ration bars were a type of food ration known by various names, including nutrient bars, protein bars, and supply bars; they were also known as sticks instead of bars. They were eaten in multiple eras of galactic history.\n",
      "- Figg & Associates Bank and Trust was a banking conglomerate. They were known to give loans to entrepreneurs, capturing the spirit of Ecclessis Figg.\n",
      "- Redirects from alternate names\n",
      "===========\n",
      "Question: What can you tell me about the First Battle of Geonosis?\n",
      "Retrieved documents:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585d67161dbe41468057ea14459cf258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a154c9f5ff44d285502822feff2496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The First Battle of Geonosis, also referred to as the Battle of Geonosis or the Battle on Geonosis, was the first major battle fought in 22 BBY between the Confederacy of Independent Systems and the Galactic Republic on Geonosis, marking the beginning of the three-year Clone Wars. It would be the first major combat of the Grand Army of the Republic, as well as the first major battle the Jedi had fought in years. The battle also caused the death of the notorious bounty hunter Jango Fett and the discovery of Count Dooku's dark side allegiance.\n",
      "- Ronto was a clone trooper copilot who fought in the First Battle of Geonosis during the Clone Wars. During his time on Geonosis, Ronto flew in a LAAT/i with Skifter.\n",
      "- The Battle of Rendili was a battle of the Clone Wars, occurring thirty months after the First Battle of Geonosis, in 20 BBY.\n",
      "- The First Battle of Kamino, also known as the Defense of Kamino, the Assault on Kamino, or simply as the Battle of Kamino, took place two months after the First Battle of Geonosis.\n",
      "- The First Battle of Tatooine was initiated by the Confederacy of Independent Systems during the Clone Wars, one month after the Battle of Geonosis.\n",
      "===========\n",
      "Question: What is the name of the settlement where Anakin Skywalker and his mother lived?\n",
      "Retrieved documents:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bb02ef6db7443081c72ead9b6b0b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbb52b0d077497bb1d44747e174c257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Mos Espa was a spaceport settlement located on the desert world of Tatooine. The settlement included a number of commercial and workspace settings, as well as entertainment establishments such as the Mos Espa Grand Arena. During the Invasion of Naboo, Mos Espa was also home to a number of slaves, including Anakin Skywalker and his mother, Shmi.\n",
      "- Finn's mother lived with him on Coruscant. In 32 BBY, she helped Anakin Skywalker return to the Jedi Temple after he helped her son repair their malfunctioning nanny droid.\n",
      "- The Skywalker home was where the slaves Anakin Skywalker and his mother Shmi Skywalker Lars lived in Mos Espa on the desert planet Tatooine. By the time the handmaiden Sabé traveled to Tatooine on behalf of Padmé Amidala to free Shmi from slavery, Shmi no longer lived in the home and the door had a newly cut symbol of a white sun on it.Queen's Shadow\n",
      "- Kovit was a settlement, nestled within a steep mountain range on Trulalis where Jaalib Brandl and his mother once lived. It was a \"gated community,\" a farming settlement dominated by a towering, butressed theatre of white limestone.\n",
      "- Mojyn was the daughter of Brandeh and lived on Lanteeb during the Clone Wars. Her mother was killed during the Battle of Lanteeb when Jedi Master Obi-Wan Kenobi and Jedi Knight Anakin Skywalker were on the planet to try and stop Separatist General Lok Durd.\n",
      "===========\n",
      "Question: Which planet did Darth Sidious represent as senator?\n",
      "Retrieved documents:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05610807ad79442bbfe8407da77a9852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484e561d8b0149d1b8ae1ae8498c356c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The massacre at the Gran Protectorate Embassy took place in 52 BBY. Senator Pax Teem of the Gran Protectorate was murdered in the embassy by Darth Plagueis' apprentice, Darth Sidious, thus removing an opponent to the Sith Lords.\n",
      "- Dawk was a planet in the Outer Rim Territories that was orbited by a moon. During the Republic Era, between around 40 BBY and 32 BBY,The events of Darth Maul &ndash; Black, White & Red 3 take place during Darth Maul's time as the Sith apprentice of Darth Sidious, which Star Wars: Timelines dates to between around 40 BBY and 32 BBY. the Devaronian criminal Coir Cion operated on the moon of Dawk. The Sith Lord Darth Sidious dispatched his Sith apprentice, Darth Maul, to kill Cion and his associates after the Devaronian threatened to blackmail Senator Sheev Palpatine&mdash;Sidious's public persona&mdash;with information on his underworld dealings.\n",
      "- The Useful Bureaucrats was a manifesto written by the Dark Lord of the Sith and Galactic Emperor Darth Sidious as part of his work, Absolute Power.  The volume detailed how Sidious, under his public persona of the Naboo Senator Palpatine, managed to manipulate several important figures and events to ensure his rise to power. He also mentioned that his Sith Master, Darth Plagueis, had a minor role in these events.\n",
      "- During the Clone Wars, Senator Padmé Amidala undertook a diplomatic mission to the planet Scipio, ostensibly to take out a loan but actually seeking to uncover evidence of corruption. She and her old flame Rush Clovis became targeted by the bounty hunter Embo, who was hired by Darth Sidious. Unbeknownst to either Amidala, Clovis, or the Senator's secret husband Jedi Knight Anakin Skywalker, Embo had been tasked not with killing any of them but with intimidating Clovis and driving him offworld, as part of Sidious' plans for the InterGalactic Banking Clan.\n",
      "- The moon of Dawk was a moon that orbited the Outer Rim planet Dawk. During the Republic Era, the moon was the territory of the fugitive Devaronian criminal Coir Cion. Between around 40 BBY and 32 BBY,The events of Darth Maul &ndash; Black, White & Red 3 take place during Darth Maul's time as the Sith apprentice of Darth Sidious, which Star Wars: Timelines dates to between around 40 BBY and 32 BBY. Cion discovered underworld dealings of Senator Sheev Palpatine&mdash;in reality, the Sith Lord Darth Sidious, who sent his Sith apprentice, Darth Maul, to kill the Devaronian.\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "retriever = BM25Retriever()\n",
    "for question in questions:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Retrieved documents:\")\n",
    "    for relevant_doc in retriever.search(question):\n",
    "        print(\"- \" + relevant_doc.replace(\"\\n\", \" \"))\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, the retrieved documents should for the most part relevant to the query. \n",
    "\n",
    "### Dense retrieval \n",
    "\n",
    "Many of those documents are, however, way too long to be included in the prompt for our Gemma model (especially if we wish to include 4-5 retrieved texts for each query!). Can we ensure that the length of each retrieved text stays within a reasonable length, such as one or two sentences? \n",
    "\n",
    "One strategy is to not return the full documents, but instead determine the most relevant _sentences_ within those documents. But how do we determine which sentence is most relevant? A sparse retriever using BM25 would not work well here, as it does not really account for the semantics of the query. Instead, what we can do is to:\n",
    "- split the documents (retrieved through BM25) into sentences\n",
    "- extract sentence embeddings for the query and for each sentence\n",
    "- compute the cosine similarities between the query vector and each sentence vector\n",
    "- and return the _k_ most similar sentences\n",
    "\n",
    "In other words, our approach starts with a _sparse retrieval step_ at the level of full documents (which we already have implemented, using BM25S), and continues with a _dense retrieval step_ to determine the most relevant sentences among the sentences that are found in the retrieved documents.\n",
    "\n",
    "**Task 3** (4 points): Re-implement the `search` method to segment into sentences each document retrieved with BM25, extract sentence embeddings for the query and sentences using the encoder model (see [here](https://sbert.net/examples/applications/semantic-search/README.html) for explanations and code examples), and then select the _k_ sentences with highest cosine similarities.  \n",
    "\n",
    "_Tips_: You can use `nltk.sent_tokenize` to segment your document in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df3c55833df46de89a27cca3a9d26c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/178246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c4662ff32c4eb3b332a53dd95b0457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/178246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de42cbecdb649789ebfa280c6d28529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/178246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khoimai/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "import re, json\n",
    "\n",
    "import sentence_transformers\n",
    "from sentence_transformers import util\n",
    "\n",
    "import nltk\n",
    "\n",
    "retriever = BM25Retriever()\n",
    "encoder = sentence_transformers.SentenceTransformer(\"msmarco-MiniLM-L-6-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9443e039fb744095bfa11b32769a58ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccc1a4f4b0a4278bf34faaa1e8e2063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([\"Luke Skywalker's lightsaber was the first lightsaber constructed by Luke Skywalker and the second one he owned.\",\n",
       "       'The Luke Skywalker X-Wing Mech was a mech piloted by Luke Skywalker. It shared a design with his X-wing and included a large lightsaber.',\n",
       "       'Holoeditor was a type of holographic technology used by Cronal to study and imitate Luke Skywalker in 5 ABY.Luke Skywalker and the Shadows of Mindor'],\n",
       "      dtype='<U148')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is Luke Skywalker?\"\n",
    "docs = retriever.search(query=query, k=3)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Luke Skywalker's lightsaber was the first lightsaber constructed by Luke Skywalker and the second one he owned.\",\n",
       " 'The Luke Skywalker X-Wing Mech was a mech piloted by Luke Skywalker.',\n",
       " 'It shared a design with his X-wing and included a large lightsaber.',\n",
       " 'Holoeditor was a type of holographic technology used by Cronal to study and imitate Luke Skywalker in 5 ABY.Luke Skywalker and the Shadows of Mindor']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sentence for doc in docs for sentence in nltk.sent_tokenize(doc)]\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = encoder.encode(retriever.corpus, convert_to_tensor=True)\n",
    "\n",
    "sentence = sentences[0]\n",
    "sentence_embedding = encoder.encode(sentence, convert_to_tensor=True)\n",
    "similarity_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;28mlen\u001b[39m(retriever\u001b[38;5;241m.\u001b[39mcorpus))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:371\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    368\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 371\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m truncate_embeddings(\n\u001b[1;32m    373\u001b[0m         out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate_dim\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:334\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    331\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/torch/nn/functional.py:1804\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1800\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m-> 1804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies a softmax function.\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m \n\u001b[1;32m   1807\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1827\u001b[0m \n\u001b[1;32m   1828\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "corpus_embeddings = encoder.encode(retriever.corpus, convert_to_tensor=True)\n",
    "\n",
    "top_k = min(5, len(retriever.corpus))\n",
    "for sentence in sentences:\n",
    "    sentence_embedding = encoder.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "    similarity_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]\n",
    "    scores, indices = torch.topk(similarity_scores, k=top_k)\n",
    "\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"Top 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(scores, indices):\n",
    "        print(retriever.corpus[idx], f\"(Score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import re, json\n",
    "\n",
    "import sentence_transformers\n",
    "from sentence_transformers import util\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class HybridRetriever(BM25Retriever):\n",
    "\n",
    "    def __init__(self,\n",
    "                 filename=\"starwars.json\", \n",
    "                 encoder_model=\"msmarco-MiniLM-L-6-v3\"):\n",
    "        \n",
    "        \"\"\"Using the json file provided as input, create a BM25s retriever \n",
    "        containing all (indexed) documents, and loads a sentence transformer model\n",
    "        used to compute the embeddings for the query and sentences\"\"\"\n",
    "\n",
    "        BM25Retriever.__init__(self, filename)\n",
    "        self.encoder = sentence_transformers.SentenceTransformer(encoder_model)\n",
    "\n",
    "    def search(self, query:str, k:int=5) -> List[str]:\n",
    "        \"\"\"Use the BM25 retriever to find the documents that are closest\n",
    "        to the provided query, and then the sentence transformer model to\n",
    "        determine the most relevant sentences\"\"\"\n",
    "\n",
    "        corpus_embeddings = encoder.encode(retriever.corpus, convert_to_tensor=True)\n",
    "\n",
    "        docs = BM25Retriever.search(self, query, k) # results from BM25\n",
    "        sentences = [sentence for doc in docs for sentence in nltk.sent_tokenize(doc)] # split the documents into sentences\n",
    "\n",
    "        top_k = min(5, len(retriever.corpus))\n",
    "        for sentence in sentences:\n",
    "            sentence_embedding = encoder.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "            similarity_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]\n",
    "            scores, indices = torch.topk(similarity_scores, k=k)\n",
    "            result = [retriever.corpus[idx] foidx in indices]\n",
    "\n",
    "            return result\n",
    "\n",
    "\n",
    "        # split the documents into sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can test our hybrid (sparse followed by dense) retriever on the same questions as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = HybridRetriever()\n",
    "for question in questions:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Retrieved documents:\")\n",
    "    for relevant_doc in retriever.search(question):\n",
    "        print(\"- \" + relevant_doc.replace(\"\\n\", \" \"))\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now that we have a functioning retriever model, we can connect it to the generative language model employed to produce the responses.\n",
    "\n",
    "**Task 4** (4 points): Implement the `RetrievalAugmentedResponseGenerator`. Given an initial input prompt, the method should first retrieve relevant sentences using the `HybridRetriever` we have just developed. Then, it should expand the initial prompt using the provided template (you are of course free to edit or adapt it as you see fit). This expanded prompt should then be tokenized and fed as input to the LLM in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PROMPT_TEMPLATE = \"'You are given the following information about Star Wars:\\n-{retrieved_sentences}\\nNow answer the following question in 1 or 2 sentences, based on the provided information: '{query}'\"\n",
    "\n",
    "class RetrievalAugmentedResponseGenerator:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemma-1.1-2b-it\", \n",
    "                 doc_filename=\"/fp/projects01/ec403/IN4080/starwars.json\", \n",
    "                 encoder_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Loads the tokenizer, pretrained causal LM for the given model, along with the \n",
    "        hybrid sparse-dense retriever model populated with the documents in doc_filename.\"\"\"\n",
    "\n",
    "        raise NotImplemented(\"You must implement this method\")\n",
    "\n",
    "    def get_response(self, query:str, max_length:int=50, k=3) -> str:\n",
    "        \"\"\"Given a prompt, retrieve k relevant sentences, generate a response (of a maximum \n",
    "        max_length tokens) and return it.\n",
    "        Only the response should be returned, not the text of the prompt itself\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplemented(\"You must implement this method\")\n",
    "\n",
    "\n",
    "agent = RetrievalAugmentedResponseGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to test our system end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for question in questions:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Answer:\", agent.get_response(question))\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5** (4 points): If you have implemented your model correctly, the system should answer correctly to at least a few questions. But it is still far from perfect, and some of the answers are flat-out wrong. Suggest 2-3 ways one could improve the current system and get even better answers. You don't need to implement anything, simply flesh out a few ideas you believe are worth trying out.\n",
    "\n",
    "_(of course, it is even better if you actually try to implement those ideas and evaluate their influence on the quality of the system responses!)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
