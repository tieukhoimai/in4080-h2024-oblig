{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080: obligatory assignment 4\n",
    " \n",
    "The final mandatory assignment for IN4080 consists of two parts. The first is about the development of dialogue systems, and the second about machine translation.\n",
    "You are required to get at least 12/20 points to pass. \n",
    "\n",
    "- We assume that you have read and are familiar with IFI’s requirements and guidelines for mandatory assignments, see [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html) and [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html).\n",
    "- This is an individual assignment. You should not deliver joint submissions. \n",
    "- You may redeliver in Devilry before the deadline (__Sunday, November 3 at 23:59__).\n",
    "- Only the last delivery will be read! If you deliver more than one file, put them into a zip-archive. You don't have to include in your delivery the data files already provided for this assignment. \n",
    "- Name your submission _your\\_username\\_in4080\\_mandatory\\_4_\n",
    "\n",
    "Part 1 should be done on your local computer, as it relies on a speech interface that will not work on remote machines. For Part 2, using _Fox_ is preferable, at least for the fine-tuning task.\n",
    "\n",
    "You should deliver a completed version of this Jupyter notebook, containing both your code and explanations about the steps you followed. We want to stress that simply submitting code is __not__ by itself sufficient to complete the assignment - we expect the notebook to also contain explanations of what you have implemented, along with motivations for the choices you made along the way. Preferably use whole sentences, and mathematical formulas if necessary. Explaining in your own words (using concepts we have covered through in the lectures) what you have implemented and reflecting on your solution is an important part of the learning process - take it seriously!\n",
    "\n",
    "Regarding the use of LLMs (ChatGPT or similar): you are allowed to use them as 'sparring partner', for instance to clarify something you have not understood. However, you are __not__ allowed to use them to generate solutions (either in part or in full) to the assignment tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Dialogue systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective in this part is to build a spoken conversational interface for a (simulated) elevator. \n",
    "\n",
    "### Basic setup\n",
    "\n",
    "First, let's make sure that we have all the necessary Python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets pyaudio openai-whisper pyttsx3 setfit spacy jellyfish\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the simulated elevator is provided below. The elevator is displayed using simple widgets (where the current floor is shown in green). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import time, random, string, threading\n",
    "from typing import List, Tuple, Dict, Set\n",
    "\n",
    "class BasicElevator:\n",
    "    \"\"\"Elevator simulated using a GUI\"\"\"\n",
    "    \n",
    "    def __init__(self, start_floor:int =1, nb_floors=10):\n",
    "        \"\"\"Initialised a new elevator, placed on the first floor\"\"\"\n",
    "        \n",
    "        # Current floor of the elevator\n",
    "        self.cur_floor: int = start_floor\n",
    "\n",
    "        # (Possibly empty) list of next floor stops to reach \n",
    "        self.next_stops : List[int] = []\n",
    "        \n",
    "        # Building the basic GUI showing the elevator\n",
    "        display(self._build_gui(nb_floors))\n",
    "\n",
    "        # Starts the thread executing the movements\n",
    "        thread = threading.Thread(target=self.elevator_move_thread)\n",
    "        thread.start()     \n",
    "            \n",
    "    def move_to_floor(self, floor_number : int):\n",
    "        \"\"\"Move to a given floor (by adding it to a stack of floors to reach)\"\"\"\n",
    "        \n",
    "        if floor_number < 1 or floor_number > len(self.floors):\n",
    "            raise RuntimeError(\"Floor number must be between 1 and %i\"%len(self.floors))\n",
    "\n",
    "        self.next_stops.append(floor_number)\n",
    "        \n",
    "        \n",
    "    def stop(self):\n",
    "        \"\"\"Stops all movements of the elevator\"\"\"\n",
    "        self.next_stops.clear()\n",
    "\n",
    "    def _build_gui(self, nb_floors):\n",
    "        \"\"\"Creates the GUI for the elevator, with a status label and a visual representation\n",
    "        of the floors, where the current floor is indicated in green.\"\"\"\n",
    "\n",
    "        # Displaying the current status of the elevator (still or going up or down)\n",
    "        status_label = widgets.HTML(\"<b>Status</b>: \")\n",
    "        self.status = widgets.Label(\"Still\")\n",
    "        status_box = widgets.HBox([status_label, self.status])\n",
    "\n",
    "        # Displaying the floors on a vertical axis\n",
    "        self.floors = []\n",
    "        floor_layout = widgets.Layout(width='50px', height='30px', border='2px solid black',justify_content=\"center\")\n",
    "        for i in range(1, nb_floors+1):\n",
    "            floor = widgets.Label(value=str(i), layout=floor_layout)\n",
    "            floor.style = {\"background\":(\"white\" if i!=self.cur_floor else \"lightgreen\")}\n",
    "            self.floors.append(floor)\n",
    "\n",
    "        # Create a vertical box container to hold the boxes\n",
    "        vbox = widgets.VBox([status_box] + self.floors[::-1])\n",
    "        return vbox\n",
    "    \n",
    "\n",
    "    def elevator_move_thread(self, speed=1.0, latency=0.1):\n",
    "        \"\"\"Trigger a movement of the elevator if the list of next stops is not \n",
    "        empty. The movement continues until all goals are reached.\"\"\"\n",
    "\n",
    "        while True:\n",
    "            while self.next_stops:\n",
    "                if self.cur_floor == self.next_stops[0]:\n",
    "                    del self.next_stops[0]\n",
    "                    continue\n",
    "                if self.cur_floor < self.next_stops[0]:\n",
    "                    next_floor = self.cur_floor+1\n",
    "                    self.status.value = \"UP\"\n",
    "                elif self.cur_floor > self.next_stops[0]:\n",
    "                    next_floor = self.cur_floor-1\n",
    "                    self.status.value = \"DOWN\"\n",
    "                time.sleep(speed)   \n",
    "                self.floors[self.cur_floor-1].style.background = \"white\"\n",
    "                self.floors[next_floor-1].style.background = \"lightgreen\"\n",
    "                self.cur_floor = next_floor\n",
    "            self.status.value = \"Still\"\n",
    "            \n",
    "            # Wait loop (until we have a goal in self.next_stops)\n",
    "            time.sleep(latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elevator can be easily controlled through the functions `move_to_floor` and `stop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3ff25b73fb4b1c83a1eafef691b6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='<b>Status</b>: '), Label(value='Still'))), Label(value='10', layout=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "elevator = BasicElevator()\n",
    "elevator.move_to_floor(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make our elevator controllable through a speech interface instead of using function calls.\n",
    "\n",
    "## Speech interface\n",
    "\n",
    "First, make sure that you have installed `pyaudio` (for audio processing), `whisper` (for speech recognition), and `pyttsx3` (for speech synthesis).\n",
    "\n",
    "The `TalkingElevator` class below extends the basic simulated elevator with speech input and output. \n",
    "\n",
    "Upon clicking on the recording button, speech is recorded from the user's microphone, and continues until the stop button is clicked. The speech recognition engine `Whisper` from OpenAI is then employed to transcribe the spoken input (either on GPU, if you have a GPU on your machine, or on CPU). The transcription result is then sent to the `process_input` function, which is responsible for determining the system response. \n",
    "\n",
    "We are going to focus on implementing this `process_input` method. Note this system reaction to new user inputs may comprise both verbal responses (to be uttered by the system through the `_say_to_user` method) and physical actions (through the `move_to_floor` and `stop` methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, time\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set\n",
    "import whisper, pyaudio, pyttsx3\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "class TalkingElevator(BasicElevator):\n",
    "    \"\"\"Extension of the simulated elevator with a speech interface\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Loading TTS and ASR models\", end=\"...\", flush=True)\n",
    "        self.tts_engine = pyttsx3.init()  \n",
    "        self.asr_engine = whisper.load_model(\"small.en\")\n",
    "        print(\"Done\")\n",
    "        \n",
    "        # Initializing the GUI\n",
    "        BasicElevator.__init__(self)\n",
    "\n",
    "        # Starts the dialogue\n",
    "        self.dialogue_history = []\n",
    "        self._say_to_user(\"Hi, what can I do for you today?\")\n",
    "    \n",
    "\n",
    "    def process_input(self, user_input: str, conf_score:float=1.0):\n",
    "        \"\"\"Processes the (transcribed) user input, and respond appropriately \n",
    "        (through a verbal response and possibly also an action, such as moving floors)\"\"\"\n",
    "\n",
    "        self._add_to_dialogue_history(user_input, speaker=\"user\", conf_score=conf_score)\n",
    "\n",
    "        # Dummy response. Should be replaced by the actual dialogue behaviour\n",
    "        self._say_to_user(\"Sorry, I don't understand you, pal\")\n",
    "\n",
    "    \n",
    "    def _say_to_user(self, system_response: str):\n",
    "        \"\"\"Say something back to the user, and add the dialogue turn to the history. The \n",
    "        synthesis is done using the pyttsx3 library.\"\"\"\n",
    "\n",
    "        self._add_to_dialogue_history(system_response, speaker=\"elevator\")\n",
    "        \n",
    "        # Stopping current TTS if one is active\n",
    "        try:\n",
    "            self.tts_engine.endLoop()\n",
    "        except:\n",
    "            pass\n",
    "        self.tts_engine.say(system_response)\n",
    "        self.tts_engine.runAndWait()\n",
    "\n",
    "\n",
    "    def _add_to_dialogue_history(self, turn:str , speaker:str, conf_score:float=1.0):\n",
    "        \"\"\"Adds a new (user or system) turn to the dialogue history list, and displays it\n",
    "         on the chat window displaying the turns\"\"\"\n",
    "\n",
    "        self.dialogue_history.append({\"speaker\":speaker, \"text\":turn, \n",
    "                                      \"conf_score\":conf_score, \"timesamp\":time.time()})\n",
    "        \n",
    "        self.history_area.value += \"&nbsp;<strong>%s</strong>:  %s\"%(speaker.title(), turn)\n",
    "        if conf_score < 1.0:\n",
    "            self.history_area.value += \" (%.2f)\"%(conf_score)\n",
    "        self.history_area.value += \"<br>\"\n",
    "   \n",
    "   \n",
    "    def _build_gui(self, nb_floors):\n",
    "        \"\"\"GUI for the Talking elevator, comprising (beyond the simulated elevator from \n",
    "        BasicElevator) a chat window showing the dialogue turns, and buttons to record\n",
    "        the user input. \n",
    "        The user should first click on the record button, then on stop when they have finished.\n",
    "        Once the stop button is clicked, the audio is transcribed by Whisper, and finally \n",
    "        forwarded to the process_input function.\"\"\"\n",
    "\n",
    "        core_gui = BasicElevator._build_gui(self, nb_floors)\n",
    "\n",
    "        self.frames = []\n",
    "        self.recording = False\n",
    "\n",
    "        def record(chunk_size=1024):\n",
    "            \"\"\"Record audio chunks to a buffer.\"\"\"\n",
    "            self.recording = True\n",
    "            p = pyaudio.PyAudio()\n",
    "            stream = p.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, \n",
    "                            frames_per_buffer=chunk_size)\n",
    "            while self.recording:\n",
    "                self.frames.append(stream.read(chunk_size))\n",
    "            stream.close()\n",
    "\n",
    "        def on_record_button_clicked(b):\n",
    "            \"Starts the recording\"\n",
    "            record_button.disabled=True\n",
    "            stop_button.disabled=False\n",
    "            self.frames = []  # Clear previous recordings\n",
    "            thread = threading.Thread(target=record)\n",
    "            thread.start()\n",
    "\n",
    "        def on_stop_button_clicked(b):\n",
    "            \"stops the recording, runs Whisper, and forward the result to process_input\"\n",
    "            self.recording = False\n",
    "            record_button.disabled=False\n",
    "            stop_button.disabled=True\n",
    "            audio_data = np.frombuffer(b\"\".join(self.frames), np.int16).astype(np.float32) * (1 / 32768.0)\n",
    "            output = self.asr_engine.transcribe(audio_data)\n",
    "\n",
    "            # We define the confidence score based on the log-probabilities\n",
    "            conf_score = np.exp(np.mean([segment[\"avg_logprob\"] for segment in output[\"segments\"]]))\n",
    "            # (and we push those up a bit, as the Whisper scores seem too low)\n",
    "            conf_score = min(1, conf_score*1.2)\n",
    "\n",
    "            # Finally, we process the input\n",
    "            self.process_input(output[\"text\"], conf_score)\n",
    "\n",
    "        # The record and stop buttons\n",
    "        record_button = widgets.Button(icon=\"microphone\")\n",
    "        stop_button = widgets.Button(icon=\"stop\", disabled=True)\n",
    "        record_button.on_click(on_record_button_clicked)\n",
    "        stop_button.on_click(on_stop_button_clicked)\n",
    "        \n",
    "        # The chat area\n",
    "        self.history_area = widgets.HTML(layout=widgets.Layout(width=\"600px\", height=\"300px\", \n",
    "                                                               border='1px solid black', overflow='scroll'))\n",
    "\n",
    "        # The right side of the GUI\n",
    "        right_side = widgets.VBox([widgets.Label(\"\"), self.history_area, widgets.HBox([record_button, stop_button])])\n",
    "        \n",
    "        extended_gui = widgets.HBox([core_gui, right_side])\n",
    "        return extended_gui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TTS and ASR models...Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8480b30df6ef492f9cc990f0a3068943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(HTML(value='<b>Status</b>: '), Label(value='Still'))), Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "elevator = TalkingElevator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope that the speech recognition and speech synthesis will work correctly -- if it isn't the case, do let us know ! (audio processing in Python can be quite tricky and will work differently from OS to OS). [^1]\n",
    "\n",
    "[^1]: If you are running on Linux and the TTS is not working, install the following packages on your machine: `sudo apt update && sudo apt install espeak ffmpeg libespeak1`\n",
    "\n",
    "**Note**: The current implementation reloads the TTS and ASR models every time, which means that you may run into a \"CUDA: out of memory\" error if you reinitialise the `TalkingElevator` many times. If this happens, simply restart the Python kernel, which will clear the memory on both the CPU and the GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Intent recognition\n",
    "\n",
    "We wish our talking elevator to support the following functionalities:\n",
    " \n",
    "- If the user express a wish to go to floor $X$ (where $X$ is an integer value between 1 and 10), the elevator should go to that floor. The interface should allow for several ways to express a given intent, such as \"_Please go to the $X$-th floor_\" or \"_Floor $X$, please_\".\n",
    "- The user requests can also be relative, for instance \"_Go one floor up_\".\n",
    "- The elevator should provide _grounding_ feedback to the user. For instance, it should respond \"_Ok, going to the $X$-th floor_\" after a user request to move to $X$.  \n",
    "- The elevator should handle misunderstandings and uncertainties, e.g. by requesting the user to repeat, or asking the user to confirm if the intent is uncertain (say, when its confidence score is lower than 0.5). \n",
    "- The elevator should also allow the user to ask where the office of a given employee is located. For instance, the user could ask \"_where is Erik Velldal's office?_\", and the elevator would provide a response such as \"_The office of Erik Velldal is on the 4th floor. Do you wish to go there?_\".  We provide you with the office numbers of a small set of IFI employees in the `OFFICES` dictionary (see below).\n",
    "- The elevator should also be able to inform the user about the current floor (such as replying to \"_Which floor are we on?_\" or \"_Are we on the 5th floor?_\"). \n",
    "- Finally, if the user asks the elevator to stop (or if the user says \"_no_\" after a grounding feedback \"_Ok, going to floor $X$._\"), the elevator should stop, and ask for clarification regarding the actual user intent. \n",
    "\n",
    "To implement this conversational behaviour, we will rely on a classical NLU-based approach in which we will recognise the user _intent_, and then determine a response based on the recognised intent(s). \n",
    "\n",
    "__Task 1.1__ (1 point): You first need to define a list of user intents that cover the kinds of user inputs you expect to observe in this talking elevator, such as `RequestMoveToFloor` or `Confirm`. This is a design question, and there is no obvious right or wrong answer. Define below the intents you want to cover, along with an explanation and a few examples of user inputs for each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Provide here the list of intent classes you have defined, together with an explanation and a few examples -->\n",
    "**Answer:**\n",
    "\n",
    "1. `RequestMoveToFloor`: This intent is used when the user wants to move to a specific floor. Examples: \"Please go to the 5th floor\", \"Floor 3, please\", \"I want to go to the 7th floor\".\n",
    "2. `RequestMoveRelative`: This intent is used when the user wants to move to a floor relative to the current floor. Examples: \"Go one floor up\", \"Move two floors down\", \"Take me to the floor above\".\n",
    "3. `RequestOfficeLocation`: This intent is used when the user wants to know the location of an employee's office. Examples: \"Where is Erik's office?\", \"Can you tell me where the office of Erik is?\", \"I need to know where Erik's office is\".\n",
    "4. `RequestCurrentFloor`: This intent is used when the user wants to know the current floor. Examples: \"Which floor are we on?\", \"Are we on the 5th floor?\", \"Can you tell me the current floor?\".\n",
    "5. `Confirm`: This intent is used when the user confirms the elevator's response. Examples: \"Yes\", \"That's correct\", \"I confirm\".\n",
    "6. `Stop`: This intent is used when the user wants the elevator to stop. Examples: \"Stop\", \"I want to stop\", \"Please stop\".\n",
    "7. `Repeat`: This intent is used when the user wants the elevator to repeat the last response. Examples: \"Can you repeat that?\", \"I didn't hear you\", \"What did you say?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.2__ (1 points): We wish to build a classifier any user input to a probability distribution over those intents, and start by creating a small, synthetic training set. Make a list of about 100 user utterances, each labelled with an intent defined above. You can \"make up\" those utterances yourself, or ask someone else to come with alternative formulations if you lack inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_utterances = [\n",
    "    # RequestMoveToFloor Intent\n",
    "    (\"Go to floor 1\", \"RequestMoveToFloor\"),\n",
    "    (\"Take me to floor 2\", \"RequestMoveToFloor\"),\n",
    "    (\"Please go up to the 5th floor\", \"RequestMoveToFloor\"),\n",
    "    (\"Take me to the top floor\", \"RequestMoveToFloor\"),\n",
    "    (\"I’d like to go to floor 3\", \"RequestMoveToFloor\"),\n",
    "    (\"Move to floor 4\", \"RequestMoveToFloor\"),\n",
    "    (\"Take us to the first floor\", \"RequestMoveToFloor\"),\n",
    "    (\"Bring me to the highest floor\", \"RequestMoveToFloor\"),\n",
    "    (\"Could we go to floor 6?\", \"RequestMoveToFloor\"),\n",
    "    (\"Can you take me to floor 8?\", \"RequestMoveToFloor\"),\n",
    "    (\"Please move to floor 7\", \"RequestMoveToFloor\"),\n",
    "    (\"I’d like to go down to the first floor\", \"RequestMoveToFloor\"),\n",
    "    (\"Take us up to floor 10\", \"RequestMoveToFloor\"),\n",
    "    (\"Let's go to floor 9\", \"RequestMoveToFloor\"),\n",
    "    (\"Bring me to floor 2\", \"RequestMoveToFloor\"),\n",
    "    (\"Elevator, please move to floor 3\", \"RequestMoveToFloor\"),\n",
    "    (\"Take me down to the ground floor\", \"RequestMoveToFloor\"),\n",
    "    (\"Take me up a floor\", \"RequestMoveToFloor\"),\n",
    "    (\"Let's head to floor 5\", \"RequestMoveToFloor\"),\n",
    "    (\"I want to go to floor 1\", \"RequestMoveToFloor\"),\n",
    "    (\"Take me to the basement\", \"RequestMoveToFloor\"),\n",
    "    (\"Could you take me down?\", \"RequestMoveToFloor\"),\n",
    "    (\"Let’s head down\", \"RequestMoveToFloor\"),\n",
    "    (\"Will you take me to the rooftop?\", \"RequestMoveToFloor\"),\n",
    "    (\"I'd like to visit floor 7\", \"RequestMoveToFloor\"),\n",
    "    (\"Could you stop at floor 3?\", \"RequestMoveToFloor\"),\n",
    "    (\"Take me to the middle floor\", \"RequestMoveToFloor\"),\n",
    "    (\"Bring me down to the lobby\", \"RequestMoveToFloor\"),\n",
    "    (\"Take me down to floor 2\", \"RequestMoveToFloor\"),\n",
    "    (\"Let's ride to floor 6\", \"RequestMoveToFloor\"),\n",
    "    (\"Please go up to floor 4\", \"RequestMoveToFloor\"),\n",
    "    (\"Take us down to the entrance\", \"RequestMoveToFloor\"),\n",
    "    (\"Elevator, take me to floor 8\", \"RequestMoveToFloor\"),\n",
    "    (\"Go up a floor, please\", \"RequestMoveToFloor\"),\n",
    "    (\"Take us to floor 3\", \"RequestMoveToFloor\"),\n",
    "    (\"Let's stop at floor 5\", \"RequestMoveToFloor\"),\n",
    "    (\"Move to floor 10\", \"RequestMoveToFloor\"),\n",
    "    (\"Let’s reach the top floor\", \"RequestMoveToFloor\"),\n",
    "    (\"Let's proceed to floor 4\", \"RequestMoveToFloor\"),\n",
    "    (\"Bring us to the lowest level\", \"RequestMoveToFloor\"),\n",
    "    (\"I'd like to head up to floor 9\", \"RequestMoveToFloor\"),\n",
    "    (\"Let’s go to floor 1\", \"RequestMoveToFloor\"),\n",
    "    (\"Head to the second level\", \"RequestMoveToFloor\"),\n",
    "\n",
    "    # RequestMoveRelative Intent\n",
    "    (\"Go one floor up\", \"RequestMoveRelative\"),\n",
    "    (\"Move two floors down\", \"RequestMoveRelative\"),\n",
    "    (\"Take me to the floor above\", \"RequestMoveRelative\"),\n",
    "    (\"Go up one floor\", \"RequestMoveRelative\"),\n",
    "    (\"Move down a floor\", \"RequestMoveRelative\"),\n",
    "    (\"Take me one floor down\", \"RequestMoveRelative\"),\n",
    "    (\"Go up two floors\", \"RequestMoveRelative\"),\n",
    "    (\"Move up one level\", \"RequestMoveRelative\"),\n",
    "    (\"Take me down two floors\", \"RequestMoveRelative\"),\n",
    "    (\"Move up a floor\", \"RequestMoveRelative\"),\n",
    "\n",
    "    # RequestOfficeLocation Intent\n",
    "    (\"Where is Erik's office?\", \"RequestOfficeLocation\"),\n",
    "    (\"Can you tell me where the office of Erik is?\", \"RequestOfficeLocation\"),\n",
    "    (\"I need to know where Erik's office is\", \"RequestOfficeLocation\"),\n",
    "    (\"Where is Erik Velldal's office?\", \"RequestOfficeLocation\"),\n",
    "    (\"Can you tell me where Erik Velldal's office is?\", \"RequestOfficeLocation\"),\n",
    "    (\"I need to know where Erik Velldal's office is\", \"RequestOfficeLocation\"),\n",
    "    (\"Where is the office of Erik Velldal?\", \"RequestOfficeLocation\"),\n",
    "    (\"Can you tell me where the office of Erik Velldal is?\", \"RequestOfficeLocation\"),\n",
    "    (\"I need to know where the office of Erik Velldal is\", \"RequestOfficeLocation\"),\n",
    "\n",
    "    # RequestCurrentFloor Intent\n",
    "    (\"Which floor are we on?\", \"RequestCurrentFloor\"),\n",
    "    (\"Are we on the 5th floor?\", \"RequestCurrentFloor\"),\n",
    "    (\"Can you tell me the current floor?\", \"RequestCurrentFloor\"),\n",
    "    (\"What floor are we on?\", \"RequestCurrentFloor\"),\n",
    "    (\"Are we on floor 5?\", \"RequestCurrentFloor\"),\n",
    "    (\"Which floor is this?\", \"RequestCurrentFloor\"),\n",
    "    (\"What floor is this?\", \"RequestCurrentFloor\"),\n",
    "    (\"Can you tell me which floor we are on?\", \"RequestCurrentFloor\"),\n",
    "    (\"Are we on the fifth floor?\", \"RequestCurrentFloor\"),\n",
    "\n",
    "    # Confirm Intent\n",
    "    (\"Yes\", \"Confirm\"),\n",
    "    (\"That's correct\", \"Confirm\"),\n",
    "    (\"I confirm\", \"Confirm\"),\n",
    "    (\"Sure\", \"Confirm\"),\n",
    "    (\"Absolutely\", \"Confirm\"),\n",
    "    (\"Correct\", \"Confirm\"),\n",
    "    (\"Indeed\", \"Confirm\"),\n",
    "    (\"Affirmative\", \"Confirm\"),\n",
    "    (\"Right\", \"Confirm\"),\n",
    "\n",
    "    # Stop Intent\n",
    "    (\"Stop\", \"Stop\"),\n",
    "    (\"I want to stop\", \"Stop\"),\n",
    "    (\"Please stop\", \"Stop\"),\n",
    "    (\"Can you stop here?\", \"Stop\"),\n",
    "    (\"Stop the elevator\", \"Stop\"),\n",
    "    (\"Stop moving\", \"Stop\"),\n",
    "    (\"Stop at the next floor\", \"Stop\"),\n",
    "    (\"Stop the elevator at once\", \"Stop\"),\n",
    "    (\"End the ride\", \"Stop\"),\n",
    "    (\"Stop right here\", \"Stop\"),\n",
    "    (\"Stop the lift\", \"Stop\"),\n",
    "    (\"Can you wait here?\", \"Stop\"),\n",
    "    (\"Stop immediately\", \"Stop\"),\n",
    "    (\"Let’s halt here\", \"Stop\"),\n",
    "    (\"Stop the elevator for me\", \"Stop\"),\n",
    "    (\"Elevator, stop now\", \"Stop\"),\n",
    "    (\"Can you pause here?\", \"Stop\"),\n",
    "\n",
    "    # Repeat Intent\n",
    "    (\"Can you repeat that?\", \"Repeat\"),\n",
    "    (\"I didn't hear you\", \"Repeat\"),\n",
    "    (\"What did you say?\", \"Repeat\"),\n",
    "    (\"Could you say that again?\", \"Repeat\"),\n",
    "    (\"Please repeat\", \"Repeat\"),\n",
    "    (\"Say that again\", \"Repeat\"),\n",
    "    (\"Repeat that\", \"Repeat\"),\n",
    "    (\"I missed that\", \"Repeat\"),\n",
    "    (\"Can you say that again?\", \"Repeat\"),\n",
    "\n",
    "    # OutOfCoverage Intent\n",
    "    (\"I don’t need a ride\", \"OutOfCoverage\"),\n",
    "    (\"Close the door\", \"OutOfCoverage\"),\n",
    "    (\"Let's take the stairs\", \"OutOfCoverage\"),\n",
    "    (\"How many floors are there?\", \"OutOfCoverage\"),\n",
    "    (\"Are you on a break?\", \"OutOfCoverage\"),\n",
    "    (\"I don't need the elevator\", \"OutOfCoverage\"),\n",
    "    (\"Is the elevator moving fast?\", \"OutOfCoverage\"),\n",
    "    (\"Are you stopping at every floor?\", \"OutOfCoverage\"),\n",
    "    (\"Is anyone else here?\", \"OutOfCoverage\"),\n",
    "    (\"I really like the IN4080 course\", \"OutOfCoverage\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train an intent classifier based on the labelled utterances you have defined. To do so, we will rely on the [SetFit](https://huggingface.co/docs/setfit/index) library, which allows one to easily train a text classification model from few examples by fine-tuning a sentence-transformer model (like the ones we used in oblig 2 and 3). Make sure that the `setfit` library is installed (`pip install setfit`).\n",
    "\n",
    "Read the [Setfit quickstart guide](https://huggingface.co/docs/setfit/quickstart) to find out how to use the library.\n",
    "\n",
    "__Task 1.3__ (2 points): Implement the `__init__`, `train` and `get_intent_distrib` methods of the `IntentClassifier` class below. The classifier should rely on a `Setfit` model trained on the labelled utterances you have already defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setfit\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "import datasets\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class IntentClassifier:\n",
    "\n",
    "    def __init__(self, model_name=\"sentence-transformers/paraphrase-mpnet-base-v2\"):\n",
    "        \"\"\"Initializes the SetFit model for intent recognition.\"\"\"\n",
    "        self.model = SetFitModel.from_pretrained(model_name)\n",
    "        self.id2label = {}  # To store mapping from label IDs to label names\n",
    "        self.label2id = {}  # To store mapping from label names to label IDs\n",
    "        self.trainer = None  # Placeholder for the trainer\n",
    "\n",
    "    def train(self, labelled_utterances: List[Tuple[str, str]]):\n",
    "        \"\"\"Trains the SetFit model on the labelled utterances.\"\"\"\n",
    "\n",
    "        # Extract unique labels for the intents and create label mappings\n",
    "        # Extract unique labels for the intents and create label mappings\n",
    "        intents = sorted(set(label for _, label in labelled_utterances))  # Sorted for consistent ID mapping\n",
    "        self.label2id = {intent: i for i, intent in enumerate(intents)}\n",
    "        self.id2label = {i: intent for intent, i in self.label2id.items()}\n",
    "\n",
    "        # Creates the dataset from the list of labelled utterances\n",
    "        train_data = datasets.Dataset.from_list([\n",
    "            {\"text\": utt, \"label\": self.label2id[label]} for utt, label in labelled_utterances\n",
    "        ])\n",
    "        \n",
    "        args = TrainingArguments(\n",
    "            batch_size=32,\n",
    "            num_epochs=10,\n",
    "        )\n",
    "        \n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=args,\n",
    "            train_dataset=train_data,\n",
    "        )\n",
    "\n",
    "        self.trainer.train()\n",
    "\n",
    "    def get_intent_distrib(self, utterance: str) -> Dict[str, float]:\n",
    "        \"\"\"Applies the trained model on a new utterance and returns a dictionary mapping\n",
    "        each intent to its probability.\"\"\"\n",
    "\n",
    "        # Get probabilities for each intent\n",
    "        probabilities = self.model.predict_proba([utterance])[0]\n",
    "\n",
    "        # Map each probability to its corresponding intent label\n",
    "        intent_distribution = {\n",
    "            self.id2label[i]: prob.item() for i, prob in enumerate(probabilities)\n",
    "        }\n",
    "        \n",
    "        return intent_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "/Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee764bee922414a97ee013fb4cff766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 10794\n",
      "  Batch size = 32\n",
      "  Num epochs = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51133acae9c6406c87c253f1608bb236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1644, 'grad_norm': 0.9339766502380371, 'learning_rate': 5.91715976331361e-08, 'epoch': 0.0}\n",
      "{'embedding_loss': 0.154, 'grad_norm': 0.7327700853347778, 'learning_rate': 2.958579881656805e-06, 'epoch': 0.15}\n",
      "{'embedding_loss': 0.1145, 'grad_norm': 0.6400842666625977, 'learning_rate': 5.91715976331361e-06, 'epoch': 0.3}\n",
      "{'embedding_loss': 0.0636, 'grad_norm': 0.34833046793937683, 'learning_rate': 8.875739644970414e-06, 'epoch': 0.44}\n",
      "{'embedding_loss': 0.0259, 'grad_norm': 0.4053441286087036, 'learning_rate': 1.183431952662722e-05, 'epoch': 0.59}\n",
      "{'embedding_loss': 0.0106, 'grad_norm': 0.1423678994178772, 'learning_rate': 1.4792899408284025e-05, 'epoch': 0.74}\n",
      "{'embedding_loss': 0.003, 'grad_norm': 0.09258869290351868, 'learning_rate': 1.7751479289940828e-05, 'epoch': 0.89}\n",
      "{'embedding_loss': 0.0022, 'grad_norm': 0.04092833027243614, 'learning_rate': 1.9921104536489153e-05, 'epoch': 1.04}\n",
      "{'embedding_loss': 0.0014, 'grad_norm': 0.38625091314315796, 'learning_rate': 1.9592373438527285e-05, 'epoch': 1.18}\n",
      "{'embedding_loss': 0.0011, 'grad_norm': 0.023442577570676804, 'learning_rate': 1.9263642340565417e-05, 'epoch': 1.33}\n",
      "{'embedding_loss': 0.001, 'grad_norm': 0.016648704186081886, 'learning_rate': 1.8934911242603552e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dda2ac45205466d9019baa98e76ee24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0008, 'grad_norm': 0.020363610237836838, 'learning_rate': 1.8606180144641684e-05, 'epoch': 1.63}\n",
      "{'embedding_loss': 0.0007, 'grad_norm': 0.016298945993185043, 'learning_rate': 1.827744904667982e-05, 'epoch': 1.78}\n",
      "{'embedding_loss': 0.0007, 'grad_norm': 0.01924309879541397, 'learning_rate': 1.794871794871795e-05, 'epoch': 1.92}\n",
      "{'embedding_loss': 0.0005, 'grad_norm': 0.010470970533788204, 'learning_rate': 1.7619986850756083e-05, 'epoch': 2.07}\n",
      "{'embedding_loss': 0.0007, 'grad_norm': 0.008277916349470615, 'learning_rate': 1.7291255752794215e-05, 'epoch': 2.22}\n",
      "{'embedding_loss': 0.0005, 'grad_norm': 0.01556310523301363, 'learning_rate': 1.696252465483235e-05, 'epoch': 2.37}\n",
      "{'embedding_loss': 0.0005, 'grad_norm': 0.009573635645210743, 'learning_rate': 1.6633793556870482e-05, 'epoch': 2.51}\n",
      "{'embedding_loss': 0.0005, 'grad_norm': 0.018516333773732185, 'learning_rate': 1.6305062458908614e-05, 'epoch': 2.66}\n",
      "{'embedding_loss': 0.0005, 'grad_norm': 0.011742454022169113, 'learning_rate': 1.5976331360946746e-05, 'epoch': 2.81}\n",
      "{'embedding_loss': 0.0004, 'grad_norm': 0.00890334602445364, 'learning_rate': 1.5647600262984878e-05, 'epoch': 2.96}\n",
      "{'embedding_loss': 0.0004, 'grad_norm': 0.015076984651386738, 'learning_rate': 1.5318869165023013e-05, 'epoch': 3.11}\n"
     ]
    }
   ],
   "source": [
    "classifier = IntentClassifier()\n",
    "classifier.train(labelled_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confirm': 0.001251610362192498,\n",
       " 'OutOfCoverage': 0.0012193290092969659,\n",
       " 'Repeat': 0.0012298758593317044,\n",
       " 'RequestCurrentFloor': 0.0012648997132574955,\n",
       " 'RequestMoveRelative': 0.0012447928632440548,\n",
       " 'RequestMoveToFloor': 0.9912965930143235,\n",
       " 'RequestOfficeLocation': 0.0012253039149202332,\n",
       " 'Stop': 0.0012675952634335391}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_intent_distrib(\"go to floor 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have any test data, we cannot really conduct an evaluation of the classification performance, but this step would be of course strongly adviced when developing a real system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slot filling\n",
    "\n",
    "In addition to the intents themselves, we also wish to detect some slots, such as floor numbers or person names. For this step, we will not use a data-driven model, but rather rely on an old-fashioned, rule-based approach:\n",
    "- For floor numbers, we will rely on string matching (with regular expressions or basic string search) that detect patterns such as \"X floor\" (where X is [first,second, third, fourth, fifth, sixth, seventh, eighth, ninth, tenth]) or \"floor X\" (where X is between 1 and 10).\n",
    "- For person names, we have a predefined list of person names to detect (employees at IFI), and we should simply search for their occurrence in the user input. The simplest implementation is to just for look for exact occurrences. However, since speech recognition will often struggle to recognize foreign person names, an even better approach would be to search for names that are phonetically close (you can use the `jellyfish` library for this).\n",
    "\n",
    "The results of the slot filling should be a dictionary mapping slot names to a canonical form of the slot value. For instance, if the utterance contains the expression \"ninth floor\", the resulting slot dictionary should be `{\"floor_number\":9}`. Similarly, the `employee_name` slot should be a name present in `OFFICES` dictionary. \n",
    "\n",
    "__Task 1.4__ (2 points): Implement the method `fill_slots` that will detect the occurrence of those slots in the user input.<br>\n",
    "(+ 1 bonus point if you implement a fuzzy matching strategy to find person names that are phonetically close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Floor numbers for a subset of the IFI employees\n",
    "OFFICES = {'Adín Ramírez Rivera': 4, 'Andreas Austeng': 4, 'Anne H Schistad Solberg': 4, \n",
    "           'Arild Torolv Søetorp Waaler': 9, 'Audun Jøsang': 9, 'Birthe Soppe': 4, 'Carsten Griwodz': 4,\n",
    "           'Dag Sjøberg': 9, 'Dag Trygve Eckhoff Wisland': 5, 'Einar Broch Johnsen': 8, \n",
    "           'Eric Bartley Jul': 10, 'Erik Velldal': 4, 'Henrik Skaug Sætra': 7, 'Ingrid Chieh Yu': 8,\n",
    "           'Jørn Anders Braa': 6, 'Kristin Bråthen': 4, 'Kyrre Glette': 4, 'Lars Groth': 6, \n",
    "           'Lilja Øvrelid': 4, 'Maja Van Der Velden': 7, 'Martin Giese': 9, 'Michael Welzl': 5, \n",
    "           'Miria Grisot': 6, 'Nils Gruschka': 9, 'Olaf Owe': 9, 'Ole Christian Lingjærde': 4, \n",
    "           'Ole Hanseth': 6, 'Paulo Ferreira': 10, 'Philipp Dominik Häfliger': 5, 'Philipp Häfliger': 5, \n",
    "           'Roman Vitenberg': 4, 'Silvia Lizeth Tapia Tarifa': 8, 'Stephan Oepen': 4, \n",
    "           'Sundeep Sahay': 6, 'Thomas Peter Plagemann': 4, 'Tone Bratteteig': 7, 'Torbjørn Rognes': 8, \n",
    "           'Truls Erikson': 6, 'Viktoria Stray': 10, 'Yngvar Berg': 5, 'Yves Scherrer': 4, \n",
    "           'Özgü Mira Alay-Erduran': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/khoimai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jellyfish\n",
    "from typing import Dict, Union\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def fuzzy_matching_name(user_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Fuzzy matches the user input to the closest employee name in the OFFICES dictionary.\n",
    "    Returns the best match if the similarity score is above 0.75, otherwise returns None.\n",
    "    \"\"\"\n",
    "    best_match, highest_score = None, 0\n",
    "\n",
    "    # Remove stopwords from user input\n",
    "    input_parts = [word for word in user_input.split() if word.lower() not in stop_words]\n",
    "    possible_name_string = \" \".join(input_parts)\n",
    "\n",
    "    # Extract potential names from the cleaned input\n",
    "    possible_names = re.findall(r'[A-Z][a-z]*\\s*[A-Z]*[a-z]*', possible_name_string)\n",
    "\n",
    "    # Calculate similarity for each possible name component\n",
    "    for possible_name in possible_names:\n",
    "        for name in OFFICES.keys():\n",
    "            match_score = jellyfish.jaro_winkler_similarity(possible_name, name)\n",
    "            # Update the best match if score is higher\n",
    "            if match_score > highest_score:\n",
    "                best_match, highest_score = name, match_score\n",
    "\n",
    "    return best_match if highest_score > 0.7 else None\n",
    "\n",
    "def fill_slots(user_input: str) -> Dict[str, Union[int, str]]:\n",
    "    \"\"\"Extracts the set of slots detected in the user inputs. More precisely, the method\n",
    "    should detect both floor numbers and person names, and return a dictionary mapping slot \n",
    "    names (in this case either `floor_number` or `employee_name`) to its corresponding\n",
    "    value, in canonical form (integer for the floor number, string for the employee name)\"\"\"\n",
    "\n",
    "    slots = {}\n",
    "\n",
    "    # Mapping words to floor numbers\n",
    "    floor_word_to_number = {\n",
    "        \"first\": 1, \"second\": 2, \"third\": 3, \"fourth\": 4, \"fifth\": 5,\n",
    "        \"sixth\": 6, \"seventh\": 7, \"eighth\": 8, \"ninth\": 9, \"tenth\": 10\n",
    "    }\n",
    "\n",
    "    # Regular expressions to detect \"floor X\" or \"X floor\" patterns\n",
    "    floor_pattern = re.compile(r\"\\b(floor\\s+(\\d+)|(\\d+)\\s+floor|({}))\\b\".format(\n",
    "        \"|\".join(floor_word_to_number.keys())), re.IGNORECASE)\n",
    "\n",
    "    # Search for floor patterns in the input\n",
    "    floor_match = floor_pattern.search(user_input)\n",
    "    if floor_match:\n",
    "        if floor_match.group(2):  # Matches \"floor X\"\n",
    "            slots[\"floor_number\"] = int(floor_match.group(2))\n",
    "        elif floor_match.group(3):  # Matches \"X floor\"\n",
    "            slots[\"floor_number\"] = int(floor_match.group(3))\n",
    "        elif floor_match.group(4):  # Matches words like \"first\", \"second\", etc.\n",
    "            slots[\"floor_number\"] = floor_word_to_number[floor_match.group(4).lower()]\n",
    "\n",
    "    # Search for exact matches of employee names\n",
    "    exact_matches = [name for name in OFFICES.keys() if name.lower() in user_input.lower()]\n",
    "    if exact_matches:\n",
    "        slots[\"employee_name\"] = exact_matches[0]\n",
    "    else:\n",
    "        fuzzy_match = fuzzy_matching_name(user_input)\n",
    "        if fuzzy_match:\n",
    "            slots[\"employee_name\"] = fuzzy_match\n",
    "\n",
    "    return slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'floor_number': 9}\n",
      "{'employee_name': 'Andreas Austeng'}\n",
      "{'floor_number': 3}\n",
      "{'employee_name': 'Olaf Owe'}\n",
      "{'employee_name': 'Andreas Austeng'}\n",
      "{'floor_number': 9}\n",
      "{'floor_number': 9}\n",
      "{'employee_name': 'Olaf Owe'}\n",
      "{'employee_name': 'Andreas Austeng'}\n"
     ]
    }
   ],
   "source": [
    "# Test cases for the fill_slots function\n",
    "\n",
    "print(fill_slots(\"Could you take me to the ninth floor?\"))  \n",
    "print(fill_slots(\"I would like to visit Andreas Austeng\"))  \n",
    "print(fill_slots(\"Take me to floor 3\"))  \n",
    "print(fill_slots(\"Where is Olaf Owe located?\")) \n",
    "print(fill_slots(\"Could you tell me where Andreas Austeng's office is?\"))  \n",
    "print(fill_slots(\"Take me to floor 9\"))\n",
    "print(fill_slots(\"Take me to floor ninth\"))  \n",
    "print(fill_slots(\"Where is Olaf Owe?\"))\n",
    "print(fill_slots(\"I'd like to see Andreas Austen\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'employee_name': 'Andreas Austeng'}\n",
      "{'employee_name': 'Andreas Austeng'}\n",
      "{'employee_name': 'Olaf Owe'}\n",
      "{'employee_name': 'Olaf Owe'}\n",
      "{'employee_name': 'Olaf Owe'}\n",
      "{'employee_name': 'Philipp Häfliger'}\n",
      "{'employee_name': 'Philipp Häfliger'}\n",
      "{'employee_name': 'Philipp Häfliger'}\n",
      "{'employee_name': 'Eric Bartley Jul'}\n",
      "{'employee_name': 'Eric Bartley Jul'}\n",
      "{'employee_name': 'Eric Bartley Jul'}\n"
     ]
    }
   ],
   "source": [
    "# Test case for fuzzy matching\n",
    "\n",
    "# 'Andreas Austeng'\n",
    "print(fill_slots(\"I'd like to visit Andreas Austen\"))\n",
    "print(fill_slots(\"I want to speak with Andres Austeng\"))\n",
    "\n",
    "# 'Olaf Owe'\n",
    "print(fill_slots(\"Where is Olaf Oh's office located?\"))\n",
    "print(fill_slots(\"Please tell me about Olaf Owa\"))\n",
    "print(fill_slots(\"Could you find Olaf Aw's office?\"))\n",
    "\n",
    "# 'Philipp Häfliger'\n",
    "print(fill_slots(\"Where can I find Phillip Hafliger?\"))\n",
    "print(fill_slots(\"I need to talk to Philip Heffliger\"))\n",
    "print(fill_slots(\"Could you direct me to Phillip Hefligar?\"))\n",
    "\n",
    "# 'Eric Bartley Jul'\n",
    "print(fill_slots(\"Is Erik Bartly in his office?\"))\n",
    "print(fill_slots(\"Can you show me where Eric Jul's office is?\"))\n",
    "print(fill_slots(\"Please guide me to Eric Bartley Jewel\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response selection\n",
    "\n",
    "The next step is to implement the response selection mechanism. The response will depend on various factors:\n",
    "- the inferred user intents from the user utterance\n",
    "- the detected slot values in the user utterance (if any)\n",
    "- the current floor\n",
    "- the list of next floor stops that are yet to be reached\n",
    "- the dialogue history (as a list of dialogue turns).\n",
    "\n",
    "The response may consist of verbal responses (enacted by calls to `_say_to_user`) but also physical actions, represented by calls to either `move_to_floor` or `stop`. \n",
    "\n",
    "__Task 1.5__ (3 points): Implement the method `_respond`, which is responsible for selecting and executing those responses. The responses should satisfy the aforementioned conversational criteria (provide grounding feedback, use confirmations and clarification requests etc.). This method will consist in practice of many _if...then...else_ blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _respond(self, intent_distrib: Dict[str, float], slots: Dict[str, Union[int, str]]):\n",
    "    \"\"\"Given a probability distribution over possible intents, and a (possibly empty) list\n",
    "    of detected slots in the user input, decide how to react. The method should lead\n",
    "    to calls to both physical actions (move_to_floor, stop) and dialogue responses \n",
    "    (via _say_to_user).\"\"\"\n",
    "\n",
    "    # Ensure intent_distrib is a dictionary\n",
    "    if not isinstance(intent_distrib, dict):\n",
    "        raise ValueError(\"intent_distrib should be a dictionary\")\n",
    "\n",
    "    # Determine the most likely intent\n",
    "    intent = max(intent_distrib, key=intent_distrib.get)\n",
    "    confidence = intent_distrib[intent]\n",
    "\n",
    "    # Handle different intents\n",
    "    if intent == \"RequestMoveToFloor\":\n",
    "        if \"floor_number\" in slots:\n",
    "            floor_number = slots[\"floor_number\"]\n",
    "            if 1 <= floor_number <= 10:\n",
    "                self._say_to_user(f\"Ok, going to the {floor_number}th floor.\")\n",
    "                self.move_to_floor(floor_number)\n",
    "            else:\n",
    "                self._say_to_user(\"Sorry, the floor number must be between 1 and 10.\")\n",
    "        else:\n",
    "            self._say_to_user(\"Which floor would you like to go to?\")\n",
    "\n",
    "    elif intent == \"RequestMoveRelative\":\n",
    "        if \"floor_number\" in slots:\n",
    "            relative_floor = slots[\"floor_number\"]\n",
    "            target_floor = self.cur_floor + relative_floor\n",
    "            if 1 <= target_floor <= 10:\n",
    "                self._say_to_user(f\"Ok, moving {relative_floor} floors.\")\n",
    "                self.move_to_floor(target_floor)\n",
    "            else:\n",
    "                self._say_to_user(\"Sorry, that would take us out of the building's range.\")\n",
    "        else:\n",
    "            self._say_to_user(\"How many floors would you like to move?\")\n",
    "\n",
    "    elif intent == \"RequestOfficeLocation\":\n",
    "        if \"employee_name\" in slots:\n",
    "            employee_name = slots[\"employee_name\"]\n",
    "            if employee_name in OFFICES:\n",
    "                office_floor = OFFICES[employee_name]\n",
    "                self._say_to_user(f\"The office of {employee_name} is on the {office_floor}th floor. Do you wish to go there?\")\n",
    "            else:\n",
    "                self._say_to_user(f\"Sorry, I don't know where {employee_name}'s office is.\")\n",
    "        else:\n",
    "            self._say_to_user(\"Whose office are you looking for?\")\n",
    "\n",
    "    elif intent == \"RequestCurrentFloor\":\n",
    "        self._say_to_user(f\"We are currently on the {self.cur_floor}th floor.\")\n",
    "\n",
    "    elif intent == \"Confirm\":\n",
    "        if self.next_stops:\n",
    "            next_floor = self.next_stops[0]\n",
    "            self._say_to_user(f\"Ok, continuing to the {next_floor}th floor.\")\n",
    "        else:\n",
    "            self._say_to_user(\"There is no pending floor request to confirm.\")\n",
    "\n",
    "    elif intent == \"Stop\":\n",
    "        self.stop()\n",
    "        self._say_to_user(\"The elevator has been stopped. Where would you like to go?\")\n",
    "\n",
    "    elif intent == \"Repeat\":\n",
    "        if self.dialogue_history:\n",
    "            last_system_turn = next(turn for turn in reversed(self.dialogue_history) if turn[\"speaker\"] == \"elevator\")\n",
    "            self._say_to_user(f\"I said: {last_system_turn['text']}\")\n",
    "        else:\n",
    "            self._say_to_user(\"I haven't said anything yet.\")\n",
    "\n",
    "    else:\n",
    "        self._say_to_user(\"Sorry, I don't understand you, pal\")\n",
    "\n",
    "setattr(TalkingElevator, \"_respond\", _respond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "The last step is to implement the `process_input` method in the `TalkingElevator` class. The method should rely on the intent recognition, slot filling and response selection mechanism (which you have implemented in the previous steps) to react to a given user input.\n",
    "\n",
    "**Task 1.6** (1 point): Implement the `process_input` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(self, user_input: str, conf_score:float=1.0):\n",
    "    \"\"\"Processes the (transcribed) user input, and respond appropriately \n",
    "    (through a verbal response and possibly also an action, such as moving floors).\n",
    "    The method should rely on the intent classifier, slot-filling function, and\n",
    "    response selection function.\"\"\"\n",
    "\n",
    "    self._add_to_dialogue_history(user_input, speaker=\"user\", conf_score=conf_score)\n",
    "    \n",
    "    # Get the intent distribution from the classifier\n",
    "    intent_distrib = classifier.get_intent_distrib(user_input)\n",
    "    \n",
    "    # Fill the slots in the user input\n",
    "    slots = fill_slots(user_input)\n",
    "    \n",
    "    # Respond based on the intent and slots\n",
    "    self._respond(intent_distrib, slots)\n",
    "\n",
    "setattr(TalkingElevator, \"process_input\", process_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to test our talking elevator: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TTS and ASR models..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527153444f1f4495b132e93fe0d78186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(HTML(value='<b>Status</b>: '), Label(value='Still'))), Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/khoimai/Documents/uio/in4080/oblig_in4080_h2024/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "elevator = TalkingElevator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your talking elevator will mostly likely not function properly right from the start. Identify what works and what doesn't and correct the code you have developed in Tasks 1.1 - 1.6 until your system meets the specifications we have outlined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Machine translation\n",
    "\n",
    "In this part, we evaluate a pre-trained machine translation model on data from the Lord of the Rings movies and fine-tune it to improve the translation quality.\n",
    "\n",
    "### Data\n",
    "\n",
    "We provide you with two files, `lotr.detok.de` and `lotr.detok.en`, containing German and English movie subtitles. These two files constitute a so-called _parallel corpus_, i.e. each sentence/line in German corresponds to a sentence/line in English. The two files have the same number of lines and the German sentence on line $i$ corresponds to the English sentence on line $i$. The subtitles are extracted from the [OpenSubtitles-2018](https://opus.nlpl.eu/OpenSubtitles/corpus/version/OpenSubtitles) corpus.\n",
    "\n",
    "Here are the first ten lines of the two files:\n",
    "\n",
    "<style scoped>\n",
    "table {\n",
    "  font-size: 12px;\n",
    "}\n",
    "</style>\n",
    "| Nb  | German (`lotr.detok.de`)         | English (`lotr.detok.en`)      |\n",
    "|---|----------------------------------|--------------------------------|\n",
    "| 1 | Die Welt ist im Wandel. | The world is changed.   |\n",
    "| 2 | Ich spüre es im Wasser. | I feel it in the water. |\n",
    "| 3 | Ich spüre es in der Erde. | I feel it in the earth. |\n",
    "| 4 | Ich rieche es in der Luft. | I smell it in the air. |\n",
    "| 5 | Vieles, was einst war, ist verloren, da niemand mehr lebt, der sich erinnert. | Much that once was is lost. For none now live who remember it. |\n",
    "| 6 | Es begann mit dem Schmieden der Großen Ringe. | It began with the forging of the Great Rings. |\n",
    "| 7 | 3 wurden den Elben gegeben, den unsterblichen, weisesten und reinsten aller Wesen. | Three were given to the Elves: Immortal, wisest and fairest of all beings. |\n",
    "| 8 | 7 den Zwergenherrschern, großen Bergleuten und Handwerkern in ihren Hallen aus Stein. | Seven to the Dwarf-lords: Great miners and craftsmen of the mountain halls. |\n",
    "| 9 | Und 9... 9 Ringe wurden den Menschen geschenkt, die vor allem anderen nach Macht streben. | And nine nine rings were gifted to the race of Men who, above all else, desire power. |\n",
    "| 10 | Denn diese Ringe bargen die Kraft und den Willen, jedes Volk zu leiten. | For within these rings was bound the strength and will to govern each race. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "We will a pretrained machine translation model for German-to-English translation. The model is available on the HuggingFace model hub and can be used with the `transformers` library.\n",
    "\n",
    "Let us first make sure that all required modules are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers accelerate evaluate sacrebleu sacremoses sentencepiece unbabel-comet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bilingual model is called [`opus-mt-de-en`](https://huggingface.co/Helsinki-NLP/opus-mt-de-en) and has been trained by the Helsinki-NLP group. Like (almost) all HuggingFace models, it consists of a _tokenizer_ and the _sequence-to-sequence model_ properly speaking. We need to load both separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"helsinki-nlp/opus-mt-de-en\")\n",
    "translator = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"helsinki-nlp/opus-mt-de-en\")\n",
    "\n",
    "# Change \"cuda\" to \"cpu\" if you're running on a machine without GPU\n",
    "device = \"cuda\"\n",
    "translator = translator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transformers` library will automatically download the models from the HuggingFace hub the first time you run this cell, so it may take a bit longer.\n",
    "\n",
    "Let's take the first two German sentences, tokenize them, and translate them to English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer([\"Die Welt ist im Wandel.\", \"Ich spüre es im Wasser.\"], return_tensors=\"pt\", padding=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = translator.generate(**tokens.to(device), max_new_tokens=50)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.1__ (1 point):\n",
    "- What do the numbers in the `input_ids` represent?\n",
    "- What is the effect of `padding=True`? How would the data look like if padding was disabled?\n",
    "- What does `max_new_tokens` do? Why do you think it is important to set this parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "- The numbers in the `input_ids` represent the token IDs of the input tokens. Each token is represented by a unique ID.\n",
    "- The effect of `padding=True` is that the input sequences are padded to the maximum length of the batch. If padding was disabled, the input sequences would not be padded, and the input sequences would have different lengths.\n",
    "- `max_new_tokens` is the maximum number of tokens that can be generated. It is important to set this parameter to avoid generating too many tokens, which could lead to memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get actual words by running the output through the `batch_decode` function of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ We assume that you will run the translations from German to English. If you would like to work on the opposite translation direction (and feel comfortable evaluating the German output), you are welcome to do so. The corresponding bilingual model is called `opus-mt-en-de`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "Before we move on, we need to split our data. We will evaluate different models and for that we'll need test data. We will also fine-tune a model, and for that we'll need training data. The entire Lord of the Rings dataset has 9640 lines.\n",
    "\n",
    "__Task 2.2__ (1 point): Split the dataset in such a way that the **last** 1000 lines are used for testing and the remaining lines (8640) for training. Save the data under the following filenames: `lotr.train.de, lotr.train.en, lotr.test.de, lotr.test.en`. You can use Python code or other tools to perform the splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_dataset(input_file_de: str, input_file_en: str, test_size: int = 1000):\n",
    "    \"\"\"\n",
    "    Splits the given dataset into training and testing files based on the specified test size.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file_de (str): Path to the German input file.\n",
    "    - input_file_en (str): Path to the English input file.\n",
    "    - test_size (int): Number of lines to use for the test set. Default is 1000.\n",
    "\n",
    "    Saves:\n",
    "    - Training and testing files for both German and English.\n",
    "    \"\"\"\n",
    "    # Check if input files exist\n",
    "    if not os.path.exists(input_file_de) or not os.path.exists(input_file_en):\n",
    "        print(\"Error: One or both input files do not exist.\")\n",
    "        return\n",
    "\n",
    "    # Read the German file\n",
    "    with open(input_file_de, \"r\") as f:\n",
    "        lines_de = f.readlines()\n",
    "\n",
    "    # Split into train and test\n",
    "    train_de = lines_de[:-test_size]\n",
    "    test_de = lines_de[-test_size:]\n",
    "\n",
    "    # Read the English file\n",
    "    with open(input_file_en, \"r\") as f:\n",
    "        lines_en = f.readlines()\n",
    "\n",
    "    # Split into train and test\n",
    "    train_en = lines_en[:-test_size]\n",
    "    test_en = lines_en[-test_size:]\n",
    "\n",
    "    # Define output filenames\n",
    "    output_train_de = input_file_de.replace(\".detok\", \".train\")\n",
    "    output_test_de = input_file_de.replace(\".detok\", \".test\")\n",
    "    output_train_en = input_file_en.replace(\".detok\", \".train\")\n",
    "    output_test_en = input_file_en.replace(\".detok\", \".test\")\n",
    "\n",
    "    # Write the training and test sets to files\n",
    "    with open(output_train_de, \"w\") as f:\n",
    "        f.writelines(train_de)\n",
    "\n",
    "    with open(output_test_de, \"w\") as f:\n",
    "        f.writelines(test_de)\n",
    "\n",
    "    with open(output_train_en, \"w\") as f:\n",
    "        f.writelines(train_en)\n",
    "\n",
    "    with open(output_test_en, \"w\") as f:\n",
    "        f.writelines(test_en)\n",
    "        \n",
    "    print(f\"Files saved as: {output_train_de}, {output_test_de}, {output_train_en}, {output_test_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved as: lotr.train.de, lotr.test.de, lotr.train.en, lotr.test.en\n"
     ]
    }
   ],
   "source": [
    "split_dataset(\"lotr.detok.de\", \"lotr.detok.en\", test_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset Dimensions (Germany) ---\n",
      "Training set size: 8640\n",
      "Testing set size: 1000\n",
      "--- Dataset Dimensions (English) ---\n",
      "Training set size: 8640\n",
      "Testing set size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load the training and testing datasets to check the dimensions\n",
    "with open(\"lotr.train.de\", \"r\") as f:\n",
    "    train_de = f.readlines()\n",
    "\n",
    "with open(\"lotr.train.en\", \"r\") as f:\n",
    "    train_en = f.readlines()\n",
    "\n",
    "with open(\"lotr.test.de\", \"r\") as f:\n",
    "    test_de = f.readlines()\n",
    "\n",
    "with open(\"lotr.test.en\", \"r\") as f:\n",
    "    test_en = f.readlines()\n",
    "\n",
    "print(\"--- Dataset Dimensions (Germany) ---\")\n",
    "print(f\"Training set size: {len(train_de)}\")\n",
    "print(f\"Testing set size: {len(test_de)}\")\n",
    "print(\"--- Dataset Dimensions (English) ---\")\n",
    "print(f\"Training set size: {len(train_en)}\")\n",
    "print(f\"Testing set size: {len(test_en)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.3__ (1 point): What are potential risks and drawbacks of splitting the dataset in this way? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "- Temporal Bias: If the subtitles follow the chronological sequence of the movie, the last 1000 lines would represent the movie's later parts. This can introduce a bias where the model may not generalize well to the entire movie context or to other movies.\n",
    "- Loss of Diversity: Subtitles in different parts of the movie may vary in vocabulary, tone, and context (e.g., introductions vs. climax vs. resolution). Using only the last portion for testing may reduce the diversity in both the training and test sets, making evaluation less robust.\n",
    "- Overfitting to Specific Contexts: Since the lines are sequential, training on one part of the movie and testing on another may lead to overfitting on context-specific patterns, reducing the model’s generalizability to random scenes or new movies.\n",
    "- Evaluation Bias: The model’s performance on a test set derived from the end of the movie may not reflect its overall performance on the whole movie or on other subtitle datasets, as it hasn’t been exposed to different types of dialogues evenly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to translate the test set with our model.\n",
    "\n",
    "__Task 2.4__ (2 points): Create a function that loads the entire `lotr.test.de` file, translates each line with the `opus-mt-de-en` model and writes its output to a new file, one sentence per line.\n",
    "\n",
    "The easiest way to do this is to just load the entire test file into a list, tokenize and translate it, but the test set may be too large to fit on GPU memory, or it might be inefficient and slow if you use a CPU. A better alternative is to split the data into batches of 50-100 sentences and send each batch separately to the translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_file, translation_file, tokenizer, translator, batch_size=100):\n",
    "    \"\"\"Translate an input file line by line using the loaded tokenizer and translator,\n",
    "    and write the translations to output_file.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "translate(\"lotr.test.de\", \"lotr.output_opus.en\", tokenizer, translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, open the output file and check that the translations look ok. In particular, the file should contain the expected number of lines and output should be in the expected language (English or German, depending on the chosen direction).\n",
    "\n",
    "__Task 2.5__ (1 point): Open both the output file and the reference translations (`lotr.test.en` if translating from German to English) and compare the first 20 lines. How would you rate the translations of the OPUS system on a scale from 1 (incomprehensible and/or completely different meaning) to 5 (grammatically correct and meaning fully preserved)? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We can now evaluate the quality of our translations. In a first step, we perform _reference-based surface-level evaluation_  using the popular BLEU score. We can do that with the `sacrebleu` module. Below is a slightly reformatted example taken from the [SacreBLEU documentation](https://github.com/mjpost/sacrebleu/tree/master?tab=readme-ov-file#using-sacrebleu-from-python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "reference = ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.']\n",
    "hypothesis = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
    "\n",
    "bleu_scorer = BLEU()\n",
    "# BLEU can deal with multiple references per sentence, but here we only have one, so we just enclose it in another set of brackets:\n",
    "score = bleu_scorer.corpus_score(hypothesis, [reference])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.6__ (1 point): Load both the system output and the reference of your test set and compute the corpus-level BLEU score. Also compute the corpus-level chrF score. Which of the scores is higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "def evaluate_bleu(hypothesis_file, reference_file):\n",
    "\tpass\n",
    "\n",
    "evaluate_bleu(\"lotr.output_opus.en\", \"lotr.test.en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides string-based metrics, neural metrics have become increasingly popular lately, since they have been shown to correlate better with human judgements. The most popular neural metric is called COMET and it can be used with the HuggingFace `evaluate` package. The example below is from the [documentation](https://huggingface.co/spaces/evaluate-metric/comet/blob/main/README.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "comet_metric = evaluate.load('comet')\n",
    "src = [\"Dem Feuer konnte Einhalt geboten werden\", \"Schulen und Kindergärten wurden eröffnet.\"]\n",
    "hyp = [\"The fire could be stopped\", \"Schools and kindergartens were open\"]\n",
    "ref = [\"They were able to control the fire.\", \"Schools and kindergartens opened\"]\n",
    "comet_score = comet_metric.compute(predictions=hyp, references=ref, sources=src)\n",
    "print(comet_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.7__ (1 point): Adapt this code to evaluate the output of the OPUS model. Note that COMET also requires the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_comet(hypothesis_file, reference_file, source_file):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Let us see now if we can further improve the translation quality. We still haven't used the training set after all...\n",
    "\n",
    "Fine-tuning a translation model with the `transformers` library is a bit convoluted. You need the following ingredients:\n",
    "- A `Seq2SeqTrainer` object, which defines the initial model and its tokenizer, the training data, and the configuration parameters (as a `Seq2SeqTrainingArguments` object). The training process starts with the `train()` method.\n",
    "- A `Seq2SeqTrainingArguments` object, which contains the configuration parameters, such as the number of training epochs, the path for saving the fine-tuned model, the learning rate etc.\n",
    "- A `DataCollatorForSeq2Seq` object that takes care of splitting the training data into batches of appropriate size.\n",
    "- A `DatasetDict` object containing the tokenized training data. Typically, the untokenized data is loaded into a `DatasetDict` object, and the tokenization function is applied to everything inside this `DatasetDict` using the `map()` function.\n",
    "\n",
    "__Task 2.8__ (1 point): The code in the box below shows a working example using the pretrained OPUS model, but is limited to two sentence pairs. Complete the code to load the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "ds = Dataset.from_dict({\n",
    "    \"src_text\": [\"Die Welt ist im Wandel.\", \"Ich spüre es im Wasser.\"],\n",
    "    \"tgt_text\": [\"The world is changed.\", \"I feel it in the water.\"]\n",
    "})\n",
    "data = DatasetDict({\"train\": ds})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"src_text\"], text_target=examples[\"tgt_text\"], max_length=max_length, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = data.map(preprocess_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=translator)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"opus-mt-de-en-lotr\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    translator,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was fine-tuned for three epochs, and you should have three checkpoints in the `opus-mt-de-en-lotr` directory.\n",
    "\n",
    "__Task 2.9__ (1 point): Choose one of the checkpoints and use it to translate the test set. Evaluate the test set with BLEU, chrF and COMET. Note that locally saved model files (and tokenizers) can be loaded in the same way as models from the HuggingFace hub, e.g. with the following command: `transformers.AutoModelForSeq2SeqLM.from_pretrained(\"opus-mt-de-en-lotr/checkpoint-810\")`\n",
    "\n",
    "Did fine-tuning help? Did fine-tuning help? Have a look at the first rows of the files. Do you agree with the metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
